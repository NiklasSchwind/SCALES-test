{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import diags\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'ACCESS-ESM1-5'\n",
    "INDICATOR = 'tas'\n",
    "TEST_SCENARIOS = ['ssp245']\n",
    "TRAIN_SCENARIOS = [ 'ssp585','ssp534-over','1pctco2','ssp245','ssp126','flat10zec', 'flat10cdr','abrupt4xco2','ssp119','ssp460','ssp370']\n",
    "N = 30\n",
    "ML_MODEL = 'random_forest'\n",
    "\n",
    "\n",
    "MODEL_PATH = f'/mnt/PROVIDE/SCALES/cmip6-ng-inc-oceans/{MODEL}'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def weights_calculate(x0, X, tau):\n",
    "    return np.exp(np.sum((X - x0) ** 2, axis=1) / (-2 * (tau ** 2)))\n",
    "\n",
    "def local_weighted_regression(x0, X, Y, tau):\n",
    "    # add bias term\n",
    "    x0 = np.r_[1, x0]\n",
    "    X = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "    # weighted least squares\n",
    "    xw = X.T * weights_calculate(x0, X, tau)\n",
    "    theta = np.linalg.pinv(xw @ X) @ xw @ Y\n",
    "    return x0 @ theta\n",
    "\n",
    "\n",
    "def local_weighted_regression_slopes(x0, X, Y, tau):\n",
    "    \"\"\"\n",
    "    Same weighted regression as `local_weighted_regression`,\n",
    "    but returns only the slope coefficients (excluding intercept).\n",
    "    \"\"\"\n",
    "    # Add bias term consistently (same as original function)\n",
    "    x0 = np.r_[1, x0]\n",
    "    X = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "    # Weighted least squares (same as original)\n",
    "    xw = X.T * weights_calculate(x0, X, tau)\n",
    "    theta = np.linalg.pinv(xw @ X) @ xw @ Y\n",
    "\n",
    "    # Return only slopes (excluding intercept)\n",
    "    return np.squeeze(theta[1:])\n",
    "\n",
    "def parse_filename(filename: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Parses a climate model filename into its components.\n",
    "    \n",
    "    Args:\n",
    "        filename: The filename to parse\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with keys: model, scenario, ensemble, indicator, or None if parsing fails\n",
    "    \"\"\"\n",
    "    pattern = r'^(.+?)_(.+?)_(.+?)_ipcc-regions_latweight\\.csv$'\n",
    "    match = re.match(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        model, scenario_ensemble, indicator = match.groups()\n",
    "        scenario = '-'.join(scenario_ensemble.split('-')[0:-1])\n",
    "        ensemble = scenario_ensemble.split('-')[-1]\n",
    "        return {\n",
    "            'model': model,\n",
    "            'scenario': scenario,\n",
    "            'ensemble': ensemble,\n",
    "            'indicator': indicator\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def smooth_regional_indicator_timeseries(regional_indicator, bandwidth=20, is_monthly=False):\n",
    "    \"\"\"\n",
    "    Smooth each row of a (N, M) or (N, M*12) array.\n",
    "    If is_monthly=True, apply smoothing separately for each month across years.\n",
    "\n",
    "    Parameters:\n",
    "        regional_indicator : np.ndarray\n",
    "            Input array, shape (N, M) normally, or (N, M*12) if monthly.\n",
    "        \n",
    "        bandwidth : int\n",
    "            Bandwidth parameter for local_weighted_regression.\n",
    "        \n",
    "        is_monthly : bool\n",
    "            Whether the input data is monthly (N, M*12) and should be smoothed by month.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray\n",
    "            Smoothed array with same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    N, total_cols = regional_indicator.shape\n",
    "\n",
    "    # Case 1: Regular (non-monthly) smoothing\n",
    "    if not is_monthly:\n",
    "        array_x = np.arange(total_cols)\n",
    "        smoothed = np.zeros_like(regional_indicator)\n",
    "\n",
    "        for i in range(N):\n",
    "            array_y = regional_indicator[i]\n",
    "            smoothed[i] = np.array([\n",
    "                local_weighted_regression(x0, array_x, array_y, bandwidth)\n",
    "                for x0 in array_x\n",
    "            ])\n",
    "        return smoothed\n",
    "\n",
    "    # Case 2: Monthly-aware smoothing (total_cols must be multiple of 12)\n",
    "    if total_cols % 12 != 0:\n",
    "        raise ValueError(\"For monthly smoothing, number of columns must be divisible by 12.\")\n",
    "\n",
    "    M = total_cols // 12  # Number of years\n",
    "\n",
    "    smoothed = np.zeros_like(regional_indicator)\n",
    "    array_x = np.arange(M)  # x-values: 0...M-1 for each month's yearly data\n",
    "\n",
    "    for i in range(N):  # Loop over regions\n",
    "        for month in range(12):  # Loop over months\n",
    "            y_month = regional_indicator[i, month::12]  # Extract all years of this month\n",
    "\n",
    "            smoothed_month_values = np.array([\n",
    "                local_weighted_regression(x0, array_x, y_month, bandwidth)\n",
    "                for x0 in array_x\n",
    "            ])\n",
    "\n",
    "            # Place smoothed values back in correct positions\n",
    "            smoothed[i, month::12] = smoothed_month_values\n",
    "\n",
    "    return smoothed\n",
    "\n",
    "def get_baseline_filename(filename: str, path: str = \".\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Finds the baseline file for a given scenario file.\n",
    "    - For SSP scenarios: returns historical file with same ensemble and indicator\n",
    "    - For non-SSP scenarios: returns piControl file with same ensemble and indicator\n",
    "    - If no exact match exists for non-SSP: returns piControl with different ensemble but same indicator\n",
    "    \n",
    "    Args:\n",
    "        filename: The scenario filename\n",
    "        path: Directory path where files are located\n",
    "    \n",
    "    Returns:\n",
    "        The baseline filename if found, None otherwise\n",
    "    \"\"\"\n",
    "    # Parse the input filename\n",
    "    components = parse_filename(filename)\n",
    "    \n",
    "    if not components:\n",
    "        print(f\"Error: Could not parse filename '{filename}'\")\n",
    "        return None\n",
    "    \n",
    "    # Check if the scenario contains 'ssp'\n",
    "    if 'ssp' in components['scenario'].lower():\n",
    "        # For SSP scenarios, use historical\n",
    "        baseline_scenario = 'historical'\n",
    "    else:\n",
    "        # For non-SSP scenarios, use piControl\n",
    "        baseline_scenario = 'picontrol'\n",
    "    \n",
    "    # Construct the baseline filename with same ensemble\n",
    "    baseline_filename = (\n",
    "        f\"{components['model']}_{baseline_scenario}-{components['ensemble']}_\"\n",
    "        f\"{components['indicator']}_ipcc-regions_latweight.csv\"\n",
    "    )\n",
    "    \n",
    "    # Check if the baseline file exists\n",
    "    baseline_path = os.path.join(path, baseline_filename)\n",
    "    if os.path.exists(baseline_path):\n",
    "        return baseline_filename\n",
    "    \n",
    "    # If not found and it's piControl, try to find one with a different ensemble\n",
    "    if baseline_scenario == 'picontrol':\n",
    "        try:\n",
    "            all_files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "            \n",
    "            # Look for any piControl file with the same model and indicator\n",
    "            pattern = rf'^{re.escape(components[\"model\"])}_picontrol-(.+?)_{re.escape(components[\"indicator\"])}_ipcc-regions_latweight\\.csv$'\n",
    "            \n",
    "            for file in all_files:\n",
    "                match = re.match(pattern, file)\n",
    "                if match:\n",
    "                    print(f\"Info: Exact ensemble match not found. Using '{file}' with different ensemble\")\n",
    "                    return file\n",
    "            \n",
    "            print(f\"Warning: No piControl baseline file found for indicator '{components['indicator']}' in '{path}'\")\n",
    "            return None\n",
    "            \n",
    "        except (FileNotFoundError, PermissionError) as e:\n",
    "            print(f\"Error accessing path: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Warning: Baseline file '{baseline_filename}' not found in '{path}'\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_all_files(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves all filenames from a given directory.\n",
    "\n",
    "    Args:\n",
    "        path: Directory path to search for files.\n",
    "\n",
    "    Returns:\n",
    "        List of filenames.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    except (FileNotFoundError, PermissionError) as e:\n",
    "        print(f\"Error accessing path: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def filter_climate_files(\n",
    "    files: List[str],\n",
    "    scenarios: Optional[List[str]] = None,\n",
    "    ensembles: Optional[List[str]] = None,\n",
    "    indicators: Optional[List[str]] = None,\n",
    "    models: Optional[List[str]] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Filters files matching the pattern:\n",
    "    {model}_{scenario}-{ensemble}_{indicator}_ipcc-regions_latweight.csv\n",
    "\n",
    "    Args:\n",
    "        files: List of filenames to filter.\n",
    "        scenarios: List of scenarios to filter (e.g., ['ssp370', 'ssp245'])\n",
    "        ensembles: List of ensembles to filter (e.g., ['r4i1p1f1', 'r1i1p1f1'])\n",
    "        indicators: List of indicators to filter (e.g., ['pr', 'tas'])\n",
    "        models: List of models to filter (e.g., ['access-cm2', 'cesm2'])\n",
    "\n",
    "    Returns:\n",
    "        List of matching filenames.\n",
    "    \"\"\"\n",
    "    pattern = r'^(.+?)_(.+?)_(.+?)_ipcc-regions_latweight\\.csv$'\n",
    "    matching_files = []\n",
    "\n",
    "    for filename in files:\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            model, scenario_ensemble, indicator = match.groups()\n",
    "            scenario = '-'.join(scenario_ensemble.split('-')[0:-1])\n",
    "            ensemble = scenario_ensemble.split('-')[-1]\n",
    "            if scenarios and scenario not in scenarios:\n",
    "                continue\n",
    "            if ensembles and ensemble not in ensembles:\n",
    "                continue\n",
    "            if indicators and indicator not in indicators:\n",
    "                continue\n",
    "            if models and model not in models:\n",
    "                continue\n",
    "\n",
    "            matching_files.append(filename)\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "def get_baseline_filename(filename: str, files: List[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Finds the baseline file for a given scenario file.\n",
    "    - For SSP scenarios: returns historical file with same ensemble and indicator\n",
    "    - For non-SSP scenarios: returns piControl file with same ensemble and indicator\n",
    "    - If no exact match exists for non-SSP: returns piControl with different ensemble but same indicator\n",
    "\n",
    "    Args:\n",
    "        filename: The scenario filename\n",
    "        files: List of available filenames to search from\n",
    "\n",
    "    Returns:\n",
    "        The baseline filename if found, None otherwise\n",
    "    \"\"\"\n",
    "    # Parse the input filename\n",
    "    components = parse_filename(filename)\n",
    "\n",
    "    if not components:\n",
    "        print(f\"Error: Could not parse filename '{filename}'\")\n",
    "        return None\n",
    "\n",
    "    # Determine baseline scenario\n",
    "    if 'ssp' in components['scenario'].lower():\n",
    "        baseline_scenario = 'historical'\n",
    "    else:\n",
    "        baseline_scenario = 'picontrol'\n",
    "\n",
    "    # Construct expected baseline filename (same ensemble & indicator)\n",
    "    baseline_filename = (\n",
    "        f\"{components['model']}_{baseline_scenario}-{components['ensemble']}_\"\n",
    "        f\"{components['indicator']}_ipcc-regions_latweight.csv\"\n",
    "    )\n",
    "\n",
    "    # Check if exact baseline exists in provided files list\n",
    "    if baseline_filename in files:\n",
    "        return baseline_filename\n",
    "\n",
    "    # If not found and baseline is piControl, try fuzzy match (other ensemble)\n",
    "    if baseline_scenario == 'picontrol':\n",
    "        pattern = rf'^{re.escape(components[\"model\"])}_picontrol-(.+?)_' \\\n",
    "                  rf'{re.escape(components[\"indicator\"])}_ipcc-regions_latweight\\.csv$'\n",
    "\n",
    "        for file in files:\n",
    "            if re.match(pattern, file):\n",
    "                print(f\"Info: Exact ensemble match not found. Using '{file}' with different ensemble\")\n",
    "                return file\n",
    "\n",
    "        print(f\"Warning: No piControl baseline file found for indicator '{components['indicator']}'\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Warning: Baseline file '{baseline_filename}' not found in file list\")\n",
    "        return None \n",
    "    \n",
    "def reorder_columns(df):\n",
    "    # Ensure required columns exist\n",
    "    if 'time' not in df.columns or 'GLOBAL' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'time' and 'GLOBAL' columns\")\n",
    "\n",
    "    # Extract all columns except time and GLOBAL\n",
    "    middle_cols = [col for col in df.columns if col not in ['time', 'GLOBAL']]\n",
    "    \n",
    "    # Sort the region columns alphabetically\n",
    "    middle_cols_sorted = sorted(middle_cols)\n",
    "\n",
    "    # Reconstruct column order\n",
    "    new_column_order = ['time'] + middle_cols_sorted + ['GLOBAL']\n",
    "\n",
    "    # Reorder dataframe\n",
    "    return df[new_column_order]\n",
    "\n",
    "\n",
    "def process_scenarios(experiment_scenario_path, simulation_name, baseline_scenario_path = None, delete_first_years = 0, monthly_trend = False, smoothed = True):\n",
    "    \"\"\"\n",
    "    Load CMIP6 baseline (piControl) and abrupt4xco2 scenario data,\n",
    "    compute anomalies relative to the baseline scenario,\n",
    "    and return:\n",
    "        (1) global anomaly timeseries (annual, 21-year rolling mean)\n",
    "        (2) regional anomaly timeseries (monthly, 21-year rolling mean)\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "    df_experiment = pd.read_csv(experiment_scenario_path, parse_dates=[\"time\"])\n",
    "    df_experiment = reorder_columns(df_experiment)\n",
    "    df_experiment['time'] = df_experiment['time'].astype(str)\n",
    "    df_experiment['time'] = df_experiment['time'].apply(lambda x: datetime.strptime(x.split('.')[0], \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    #if 'historical' in baseline_scenario_path:\n",
    "    df_baseline = pd.read_csv(baseline_scenario_path, parse_dates=[\"time\"])\n",
    "    df_baseline = reorder_columns(df_baseline)\n",
    "    df_baseline['time'] = df_baseline['time'].astype(str)\n",
    "    df_baseline['time'] = df_baseline['time'].apply(lambda x: datetime.strptime(x.split('.')[0], \"%Y-%m-%d %H:%M:%S\"))\n",
    "  \n",
    "\n",
    "    first_year_experiment = df_experiment['time'][0].year\n",
    "    \n",
    "    if first_year_experiment < 1000: \n",
    "        first_year_experiment_shift = 1850 - first_year_experiment\n",
    "        df_experiment.time = df_experiment.time.map(lambda dt: dt.replace(year=dt.year + first_year_experiment_shift))\n",
    "        first_year_experiment = df_experiment['time'][0].year\n",
    "\n",
    "    \n",
    "    last_year_baseline = df_baseline['time'].iloc[-1].year\n",
    "    year_shift = (first_year_experiment - 1)  - int(last_year_baseline)\n",
    "    df_baseline.time = df_baseline.time.map(lambda dt: dt.replace(year=dt.year + year_shift))\n",
    "    #df_baseline['time'] = pd.to_datetime(df_experiment['time'])\n",
    "    \n",
    "    df_experiment = pd.concat([df_baseline, df_experiment]).sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    if (df_experiment['time'].iloc[-1].year - df_experiment['time'][0].year) > 450: \n",
    "        delete_additional_years = (df_experiment['time'].iloc[-1].year - df_experiment['time'][0].year) - 450\n",
    "        df_experiment = df_experiment[\n",
    "            [dt.year >= min(dt.year for dt in df_experiment['time']) + delete_additional_years for dt in df_experiment['time']]\n",
    "                ].reset_index(drop=True)\n",
    "\n",
    "        # Shift remaining years so first year becomes start_year\n",
    "        year_shift = 1750 -  df_experiment['time'][0].year\n",
    "        df_experiment['time'] = df_experiment['time'].apply(lambda dt: dt.replace(year=dt.year + year_shift))\n",
    "\n",
    "    #df_experiment['time'] = df_experiment['time'].apply(\n",
    "    #lambda x: f'{str(int(x[0])+1)}{x[1:]}' if int(x[0:3]) < 1500 else x\n",
    "    #)\n",
    "    #df_experiment['time'] = pd.to_datetime(df_experiment['time'])\n",
    "\n",
    "    first_year = df_experiment['time'][0].year\n",
    "    year_shift = 1750 - int(first_year)\n",
    "    df_experiment.time = df_experiment.time.map(lambda dt: dt.replace(year=dt.year + year_shift))\n",
    "    df_experiment['time'] = pd.to_datetime(df_experiment['time'])\n",
    "    \n",
    "    \n",
    "    region_cols = [col for col in df_experiment.columns if col.lower() != 'time']\n",
    "    \n",
    "    # calculate baseline\n",
    "    #if 'historical' not in baseline_scenario_path:\n",
    "    #    df_baseline = pd.read_csv(baseline_scenario_path, parse_dates=[\"time\"])\n",
    "    #    df_baseline = df_baseline.set_index(\"time\")\n",
    "    #    baseline_means = df_baseline[region_cols].mean()\n",
    "    #else: \n",
    "    df_baseline = copy.deepcopy(df_experiment)\n",
    "    df_baseline['time'] = df_baseline['time'].astype(str)\n",
    "    df_baseline['time'] = df_baseline['time'].apply(lambda x: datetime.strptime(x.split('.')[0], \"%Y-%m-%d %H:%M:%S\"))\n",
    "    #df_baseline['time'] = pd.to_datetime(df_baseline['time'])\n",
    "    df_baseline = df_baseline[(df_baseline['time'].dt.year >= 1850) & (df_baseline['time'].dt.year <= 1900)]\n",
    "    baseline_means = df_baseline[region_cols].mean()\n",
    "\n",
    "    if delete_first_years != 0: \n",
    "        df_experiment = df_experiment[df_experiment['time'].dt.year >= df_experiment['time'].dt.year.min() + delete_first_years].reset_index(drop=True)\n",
    "        # Shift remaining years so first year becomes start_year\n",
    "        year_shift = 1850 - df_experiment['time'].dt.year.min()\n",
    "        df_experiment['time'] = df_experiment['time'].apply(lambda dt: dt.replace(year=dt.year + year_shift))\n",
    "\n",
    "    df_experiment = df_experiment.set_index(\"time\")\n",
    "\n",
    "    # (B) Compute regional anomalies and apply 21-year (252-month) rolling mean\n",
    "    df_regional_anomaly = df_experiment[region_cols] - baseline_means\n",
    "    #df_regional_smoothed = df_regional_anomaly\n",
    "    if monthly_trend:\n",
    "        if smoothed:\n",
    "            df_regional_smoothed = (\n",
    "                df_regional_anomaly\n",
    "                .groupby(df_regional_anomaly.index.month)\n",
    "                .apply(lambda x: x.rolling(window=21, center=True).mean())\n",
    "                .reset_index(level=0, drop=True))  \n",
    "        else: \n",
    "            df_regional_smoothed = (\n",
    "                df_regional_anomaly\n",
    "                .groupby(df_regional_anomaly.index.month)\n",
    "                .apply(lambda x: x)  # just keep values as they are\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "        df_global = pd.DataFrame({\n",
    "        'time': df_experiment.index,\n",
    "        'GMT': df_regional_smoothed.GLOBAL#df_regional_smoothed.mean(axis=1)\n",
    "            }).set_index('time')\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        #df_regional_smoothed = df_regional_anomaly.rolling(window=21*12, center = True).mean()\n",
    "        df_annual = df_regional_anomaly.resample('Y').mean()\n",
    "        if smoothed:\n",
    "            df_regional_smoothed = df_annual.rolling(window=21, center=True).mean()\n",
    "        else: \n",
    "            df_regional_smoothed = df_annual.copy()\n",
    "        \n",
    "        df_global = pd.DataFrame({\n",
    "        'time': df_regional_smoothed.index,\n",
    "        'GMT': df_regional_smoothed.GLOBAL#df_regional_smoothed.mean(axis=1)\n",
    "    }).set_index('time')\n",
    "    \n",
    "   \n",
    "    # Convert monthly to annual mean\n",
    "    df_global['year'] = df_global.index.astype(str).str[:4].astype(int)\n",
    "    df_global_annual = df_global.groupby('year')['GMT'].mean().reset_index()\n",
    "    #df_global_annual = df_global_annual.set_index(\"year\")\n",
    "    \n",
    "    # Apply 21-year annual rolling mean\n",
    "    #print(df_global_annual['GMT'])\n",
    "    #df_global_annual['GMT'] = df_global_annual['GMT'].rolling(window=21, center = True).mean()\n",
    "\n",
    "    # Add simulation name\n",
    "    df_global_annual.insert(0, 'simulation_name', simulation_name)\n",
    "    \n",
    "    # Remove GMT from regional temperature timeseries\n",
    "    \n",
    "    df_regional_smoothed.drop(columns=['GLOBAL'], inplace = True)\n",
    "   \n",
    "    return df_global_annual, df_regional_smoothed\n",
    "\n",
    "def detect_is_monthly(df, gmt_df):\n",
    "    idx = df.index\n",
    "\n",
    "    # Case 1: Index is datetime → use infer_freq\n",
    "    if isinstance(idx, pd.DatetimeIndex):\n",
    "        freq = pd.infer_freq(idx)\n",
    "        return freq in ['M', 'MS']\n",
    "\n",
    "    # Case 2: Index is integer years → assume annual\n",
    "    if np.issubdtype(idx.dtype, np.integer):\n",
    "        # Annual data should have roughly same length as GMT series\n",
    "        return len(df) > len(gmt_df)\n",
    "\n",
    "    # Case 3: Index is string like '2000-01', '1999-12'\n",
    "    try:\n",
    "        idx_dt = pd.to_datetime(idx, format='%Y-%m')\n",
    "        freq = pd.infer_freq(idx_dt)\n",
    "        return freq in ['M', 'MS']\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Default fallback\n",
    "    return False\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def expand_annual_to_monthly(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Take annual data indexed by year (DatetimeIndex or year ints)\n",
    "    and return monthly data via linear interpolation.\n",
    "    Ensures first = Jan, last = Dec.\n",
    "    \"\"\"\n",
    "\n",
    "    s = series.copy()\n",
    "\n",
    "    # ---- 1. Make the index timezone-naive and normalized ----\n",
    "    if isinstance(s.index, pd.DatetimeIndex):\n",
    "\n",
    "        # If tz-aware → remove timezone\n",
    "        if s.index.tz is not None:\n",
    "            idx = s.index.tz_convert(None)\n",
    "        else:\n",
    "            idx = s.index  # already tz-naive\n",
    "\n",
    "        # Normalize (remove hour/min/sec)\n",
    "        s.index = idx.normalize()\n",
    "\n",
    "    else:\n",
    "        # e.g. Int64Index of years → convert to Timestamp\n",
    "        s.index = pd.to_datetime(s.index.astype(str) + \"-01-01\")\n",
    "\n",
    "    # ---- 2. Add Dec-31 entry for last year ----\n",
    "    first_year = s.index.year.min()\n",
    "    last_year = s.index.year.max()\n",
    "\n",
    "    start = pd.Timestamp(f\"{first_year}-01-01\")\n",
    "    end = pd.Timestamp(f\"{last_year}-12-01\")\n",
    "\n",
    "    # Ensure last year has a December timestamp\n",
    "    if end not in s.index:\n",
    "        s.loc[end] = s.loc[pd.Timestamp(f\"{last_year}-01-01\")]\n",
    "        s = s.drop(pd.Timestamp(f\"{last_year}-01-01\"))\n",
    "    # ---- 3. Create complete monthly index ----\n",
    "    full_monthly_index = pd.date_range(start=start, end=end, freq=\"MS\")\n",
    "\n",
    "    # ---- 4. Reindex & interpolate monthly ----\n",
    "    s = s.reindex(full_monthly_index).interpolate(\"linear\")\n",
    "\n",
    "    return s\n",
    "\n",
    "def process_gmt_and_regions_into_array(data_tuple: Tuple[pd.DataFrame, pd.DataFrame], weighted_linear_smoothing = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Processes a single tuple of (GMT DataFrame, regional DataFrame) into\n",
    "    a numpy array of shape (1 + number_regions) x number_timesteps.\n",
    "\n",
    "    Steps:\n",
    "    1. Remove NaN values\n",
    "    2. If regional data is monthly -> interpolate GMT to monthly\n",
    "       If regional data is annual -> keep GMT as is\n",
    "    3. Align data by time\n",
    "    4. Convert to numpy array: first row GMT, rest regional\n",
    "    \"\"\"\n",
    "    gmt_df, region_df = data_tuple\n",
    "\n",
    "    # --- 1. Clean NaN values ---\n",
    "    gmt_df = gmt_df.dropna(subset=['GMT'])\n",
    "    region_df = region_df.dropna(axis=0, how='all')  # Remove rows where all regions are NaN\n",
    "    # --- 2. Determine if regional data is monthly or annual ---\n",
    "    #freq = pd.infer_freq(region_df.index)\n",
    "    is_monthly = detect_is_monthly(region_df, gmt_df)#len(region_df.index) > len(gmt_df) or freq in ['M', 'MS'] # detect_is_monthly(region_df, gmt_df)\n",
    "\n",
    "    # --- 3. Prepare GMT time series ---\n",
    "    # Convert year to datetime for consistency\n",
    "    gmt_df['time'] = pd.to_datetime(gmt_df['year'], format='%Y')\n",
    "    gmt_ts = gmt_df.set_index('time')['GMT']\n",
    "    \n",
    "\n",
    "    if is_monthly:\n",
    "        # Create monthly index from the regional dataframe\n",
    "        monthly_index = region_df.index\n",
    "\n",
    "        # Interpolate GMT to monthly\n",
    "        gmt_monthly = expand_annual_to_monthly(gmt_ts)#gmt_ts.reindex(\n",
    "            #pd.date_range(gmt_ts.index.min(), gmt_ts.index.max(), freq='MS')\n",
    "        #).interpolate('linear')\n",
    "    \n",
    "        # Align GMT to regional timestamps (forward/backward fill allowed)\n",
    "        gmt_aligned = gmt_monthly#.reindex(monthly_index)\n",
    "    else:\n",
    "        # Assume annual data, map directly by year\n",
    "        region_df = region_df.copy()\n",
    "        region_df['year'] = region_df.index.year\n",
    "        gmt_aligned = region_df['year'].map(dict(zip(gmt_df['year'], gmt_df['GMT']))) \n",
    "\n",
    "\n",
    "    # --- 4. Merge GMT and regional data ---\n",
    "    # Ensure no NaN in GMT after alignment\n",
    "    gmt_vals = np.array(gmt_aligned.dropna()).reshape(1, -1)\n",
    "  \n",
    "    # Drop non-regional columns and convert to numpy\n",
    "    region_clean = region_df.drop(columns=[col for col in ['year'] if col in region_df.columns])\n",
    "    regional_vals = region_clean.T.values\n",
    "    \n",
    "    gmt_vals = gmt_vals[:,:regional_vals.shape[1]]\n",
    "    # --- 5. Combine GMT + Regional into final array ---\n",
    "    result_array = np.vstack([gmt_vals, regional_vals])\n",
    "\n",
    "    if weighted_linear_smoothing:\n",
    "        return smooth_regional_indicator_timeseries(regional_indicator=result_array, bandwidth=20, is_monthly=is_monthly)\n",
    "    \n",
    "    return result_array\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prepare_train_data(data,n):\n",
    "    \"\"\" \n",
    "    data: array with shape (1 + number_regions, number_timesteps)\n",
    "    n: window size length\n",
    "    \"\"\"\n",
    "    regions, timesteps = data.shape\n",
    "    first_region = data[0]   # shape: (timesteps,)\n",
    "    other_regions = data[1:] # shape: (number_regions, timesteps)\n",
    "\n",
    "    # Number of valid training windows\n",
    "    num_samples = timesteps - n - 1\n",
    "\n",
    "    # X shape target: (regions, n, num_samples)\n",
    "    X = np.zeros((regions, n, num_samples))\n",
    "\n",
    "    for i in range(num_samples):  # x goes from n to timesteps-1\n",
    "        # First region: t[x-n] ... t[x]  (length n)\n",
    "        X[0, :, i] = first_region[(i+1):(i+n+1)]\n",
    "\n",
    "        # Other regions: t[x-n-1] ... t[x-1] (also length n)\n",
    "        X[1:, :, i] = other_regions[:, i:(i+n)]\n",
    "\n",
    "    # Y shape target: (number_regions, num_samples)\n",
    "    Y = np.zeros((regions - 1, num_samples))\n",
    "\n",
    "    # At time t[x], take all region values except the first one\n",
    "    Y[:, :] = other_regions[:, (n+1):]  # n to end: timesteps - n samples\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def prepare_all_train_data(data_arrays, n):\n",
    "    \"\"\"\n",
    "    data_arrays: list of arrays, each shape (1 + number_regions, number_timesteps)\n",
    "    n: window size length\n",
    "    \"\"\"\n",
    "\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "\n",
    "    for data in data_arrays:\n",
    "        \n",
    "        X, Y = prepare_train_data(data,n)\n",
    "\n",
    "        X_list.append(X)\n",
    "        Y_list.append(Y)\n",
    "\n",
    "    # Combine multiple samples along the third dimension\n",
    "    X_combined = np.concatenate(X_list, axis=2)  \n",
    "    Y_combined = np.concatenate(Y_list, axis=1)\n",
    "\n",
    "    return X_combined, Y_combined\n",
    "\n",
    "def shuffle_train_data(X, Y, random_state=None):\n",
    "    \"\"\"\n",
    "    Shufflestraining data together so their sample alignment stays correct.\n",
    "\n",
    "    X shape: (features, n, samples)\n",
    "    Y shape: (targets, samples)\n",
    "\n",
    "    Returns: shuffled X and Y\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    num_samples = X.shape[2]\n",
    "    indices = np.random.permutation(num_samples)\n",
    "\n",
    "    X_shuffled = X[:, :, indices]   # shuffle along last axis\n",
    "    Y_shuffled = Y[:, indices]      # shuffle along last axis\n",
    "\n",
    "    return X_shuffled, Y_shuffled\n",
    "\n",
    "\n",
    "potential_files = get_all_files(MODEL_PATH)\n",
    "\n",
    "train_files = filter_climate_files(files = potential_files, scenarios = TRAIN_SCENARIOS, indicators = [INDICATOR])\n",
    "test_files = filter_climate_files(files = potential_files, scenarios = TEST_SCENARIOS, indicators = [INDICATOR])\n",
    "\n",
    "train_files_with_baseline = [(get_baseline_filename(filename=filename, files= potential_files), filename) for filename in train_files]\n",
    "test_files_with_baseline = [(get_baseline_filename(filename=filename, files= potential_files), filename) for filename in test_files]\n",
    "\n",
    "for (base,exp) in test_files_with_baseline: \n",
    "    print(base, exp)\n",
    "\n",
    "train_data_df = [process_scenarios(experiment_scenario_path = f'{MODEL_PATH}/{experiment}', simulation_name = experiment, baseline_scenario_path = f'{MODEL_PATH}/{baseline}', delete_first_years = 0, monthly_trend = False, smoothed = True) for (baseline,experiment) in train_files_with_baseline]\n",
    "test_data_df = [process_scenarios(experiment_scenario_path = f'{MODEL_PATH}/{experiment}', simulation_name = experiment, baseline_scenario_path = f'{MODEL_PATH}/{baseline}', delete_first_years = 0, monthly_trend = False, smoothed = True) for (baseline,experiment) in test_files_with_baseline]\n",
    "\n",
    "train_data_df = [process_scenarios(experiment_scenario_path = f'{MODEL_PATH}/{experiment}', simulation_name = experiment, baseline_scenario_path = f'{MODEL_PATH}/{baseline}', delete_first_years = 0, monthly_trend = False, smoothed = True) for (baseline,experiment) in train_files_with_baseline]\n",
    "test_data_df = [process_scenarios(experiment_scenario_path = f'{MODEL_PATH}/{experiment}', simulation_name = experiment, baseline_scenario_path = f'{MODEL_PATH}/{baseline}', delete_first_years = 0, monthly_trend = False, smoothed = True) for (baseline,experiment) in test_files_with_baseline]\n",
    "\n",
    "train_data_np = [process_gmt_and_regions_into_array(GMT_regional_values_tuple, weighted_linear_smoothing = False) for GMT_regional_values_tuple in train_data_df]\n",
    "test_data_np = [process_gmt_and_regions_into_array(GMT_regional_values_tuple, weighted_linear_smoothing = False) for GMT_regional_values_tuple in test_data_df]\n",
    "\n",
    "train_data_input, train_data_output = prepare_all_train_data(train_data_np, n=N)\n",
    "test_data_input, test_data_output = prepare_all_train_data(test_data_np, n=N)\n",
    "\n",
    "train_data_shuffeled = shuffle_train_data(train_data_input, train_data_output, random_state=42)\n",
    "\n",
    "test_data_for_autoregression_input, test_data_for_autoregression_output = prepare_train_data(test_data_np[0],N)\n",
    "gmt_autoregressive_test = test_data_np[0][0,:]\n",
    "autoregressive_test_groudtruth = test_data_np[0][1:,:]\n",
    "\n",
    "gmt_train = train_data_np[0][0,:]\n",
    "train_data_regional_temperature = train_data_np[0][1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'ACCESS-ESM1-5'\n",
    "INDICATOR = 'tas'\n",
    "TEST_SCENARIOS = ['ssp534-over']\n",
    "TRAIN_SCENARIOS = [ 'ssp585','ssp534-over','1pctco2','ssp245','ssp126','flat10zec', 'flat10cdr','abrupt4xco2','ssp119','ssp460','ssp370']\n",
    "N = 30\n",
    "ML_MODEL = 'xgboost' #'random_forest'\n",
    "PATTERN_SCALING_RESIDUALS = True\n",
    "\n",
    "MODEL_PATH = f'/mnt/PROVIDE/SCALES/cmip6-ng-inc-oceans/{MODEL}'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def weights_calculate(x0, X, tau):\n",
    "    return np.exp(np.sum((X - x0) ** 2, axis=1) / (-2 * (tau ** 2)))\n",
    "\n",
    "def local_weighted_regression(x0, X, Y, tau):\n",
    "    # add bias term\n",
    "    x0 = np.r_[1, x0]\n",
    "    X = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "    # weighted least squares\n",
    "    xw = X.T * weights_calculate(x0, X, tau)\n",
    "    theta = np.linalg.pinv(xw @ X) @ xw @ Y\n",
    "    return x0 @ theta\n",
    "\n",
    "\n",
    "def local_weighted_regression_slopes(x0, X, Y, tau):\n",
    "    \"\"\"\n",
    "    Same weighted regression as `local_weighted_regression`,\n",
    "    but returns only the slope coefficients (excluding intercept).\n",
    "    \"\"\"\n",
    "    # Add bias term consistently (same as original function)\n",
    "    x0 = np.r_[1, x0]\n",
    "    X = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "    # Weighted least squares (same as original)\n",
    "    xw = X.T * weights_calculate(x0, X, tau)\n",
    "    theta = np.linalg.pinv(xw @ X) @ xw @ Y\n",
    "\n",
    "    # Return only slopes (excluding intercept)\n",
    "    return np.squeeze(theta[1:])\n",
    "\n",
    "def parse_filename(filename: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Parses a climate model filename into its components.\n",
    "    \n",
    "    Args:\n",
    "        filename: The filename to parse\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with keys: model, scenario, ensemble, indicator, or None if parsing fails\n",
    "    \"\"\"\n",
    "    pattern = r'^(.+?)_(.+?)_(.+?)_ipcc-regions_latweight\\.csv$'\n",
    "    match = re.match(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        model, scenario_ensemble, indicator = match.groups()\n",
    "        scenario = '-'.join(scenario_ensemble.split('-')[0:-1])\n",
    "        ensemble = scenario_ensemble.split('-')[-1]\n",
    "        return {\n",
    "            'model': model,\n",
    "            'scenario': scenario,\n",
    "            'ensemble': ensemble,\n",
    "            'indicator': indicator\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def smooth_regional_indicator_timeseries(regional_indicator, bandwidth=20, is_monthly=False):\n",
    "    \"\"\"\n",
    "    Smooth each row of a (N, M) or (N, M*12) array.\n",
    "    If is_monthly=True, apply smoothing separately for each month across years.\n",
    "\n",
    "    Parameters:\n",
    "        regional_indicator : np.ndarray\n",
    "            Input array, shape (N, M) normally, or (N, M*12) if monthly.\n",
    "        \n",
    "        bandwidth : int\n",
    "            Bandwidth parameter for local_weighted_regression.\n",
    "        \n",
    "        is_monthly : bool\n",
    "            Whether the input data is monthly (N, M*12) and should be smoothed by month.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray\n",
    "            Smoothed array with same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    N, total_cols = regional_indicator.shape\n",
    "\n",
    "    # Case 1: Regular (non-monthly) smoothing\n",
    "    if not is_monthly:\n",
    "        array_x = np.arange(total_cols)\n",
    "        smoothed = np.zeros_like(regional_indicator)\n",
    "\n",
    "        for i in range(N):\n",
    "            array_y = regional_indicator[i]\n",
    "            smoothed[i] = np.array([\n",
    "                local_weighted_regression(x0, array_x, array_y, bandwidth)\n",
    "                for x0 in array_x\n",
    "            ])\n",
    "        return smoothed\n",
    "\n",
    "    # Case 2: Monthly-aware smoothing (total_cols must be multiple of 12)\n",
    "    if total_cols % 12 != 0:\n",
    "        raise ValueError(\"For monthly smoothing, number of columns must be divisible by 12.\")\n",
    "\n",
    "    M = total_cols // 12  # Number of years\n",
    "\n",
    "    smoothed = np.zeros_like(regional_indicator)\n",
    "    array_x = np.arange(M)  # x-values: 0...M-1 for each month's yearly data\n",
    "\n",
    "    for i in range(N):  # Loop over regions\n",
    "        for month in range(12):  # Loop over months\n",
    "            y_month = regional_indicator[i, month::12]  # Extract all years of this month\n",
    "\n",
    "            smoothed_month_values = np.array([\n",
    "                local_weighted_regression(x0, array_x, y_month, bandwidth)\n",
    "                for x0 in array_x\n",
    "            ])\n",
    "\n",
    "            # Place smoothed values back in correct positions\n",
    "            smoothed[i, month::12] = smoothed_month_values\n",
    "\n",
    "    return smoothed\n",
    "\n",
    "def get_baseline_filename(filename: str, path: str = \".\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Finds the baseline file for a given scenario file.\n",
    "    - For SSP scenarios: returns historical file with same ensemble and indicator\n",
    "    - For non-SSP scenarios: returns piControl file with same ensemble and indicator\n",
    "    - If no exact match exists for non-SSP: returns piControl with different ensemble but same indicator\n",
    "    \n",
    "    Args:\n",
    "        filename: The scenario filename\n",
    "        path: Directory path where files are located\n",
    "    \n",
    "    Returns:\n",
    "        The baseline filename if found, None otherwise\n",
    "    \"\"\"\n",
    "    # Parse the input filename\n",
    "    components = parse_filename(filename)\n",
    "    \n",
    "    if not components:\n",
    "        print(f\"Error: Could not parse filename '{filename}'\")\n",
    "        return None\n",
    "    \n",
    "    # Check if the scenario contains 'ssp'\n",
    "    if 'ssp' in components['scenario'].lower():\n",
    "        # For SSP scenarios, use historical\n",
    "        baseline_scenario = 'historical'\n",
    "    else:\n",
    "        # For non-SSP scenarios, use piControl\n",
    "        baseline_scenario = 'picontrol'\n",
    "    \n",
    "    # Construct the baseline filename with same ensemble\n",
    "    baseline_filename = (\n",
    "        f\"{components['model']}_{baseline_scenario}-{components['ensemble']}_\"\n",
    "        f\"{components['indicator']}_ipcc-regions_latweight.csv\"\n",
    "    )\n",
    "    \n",
    "    # Check if the baseline file exists\n",
    "    baseline_path = os.path.join(path, baseline_filename)\n",
    "    if os.path.exists(baseline_path):\n",
    "        return baseline_filename\n",
    "    \n",
    "    # If not found and it's piControl, try to find one with a different ensemble\n",
    "    if baseline_scenario == 'picontrol':\n",
    "        try:\n",
    "            all_files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "            \n",
    "            # Look for any piControl file with the same model and indicator\n",
    "            pattern = rf'^{re.escape(components[\"model\"])}_picontrol-(.+?)_{re.escape(components[\"indicator\"])}_ipcc-regions_latweight\\.csv$'\n",
    "            \n",
    "            for file in all_files:\n",
    "                match = re.match(pattern, file)\n",
    "                if match:\n",
    "                    print(f\"Info: Exact ensemble match not found. Using '{file}' with different ensemble\")\n",
    "                    return file\n",
    "            \n",
    "            print(f\"Warning: No piControl baseline file found for indicator '{components['indicator']}' in '{path}'\")\n",
    "            return None\n",
    "            \n",
    "        except (FileNotFoundError, PermissionError) as e:\n",
    "            print(f\"Error accessing path: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Warning: Baseline file '{baseline_filename}' not found in '{path}'\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_all_files(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves all filenames from a given directory.\n",
    "\n",
    "    Args:\n",
    "        path: Directory path to search for files.\n",
    "\n",
    "    Returns:\n",
    "        List of filenames.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    except (FileNotFoundError, PermissionError) as e:\n",
    "        print(f\"Error accessing path: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def filter_climate_files(\n",
    "    files: List[str],\n",
    "    scenarios: Optional[List[str]] = None,\n",
    "    ensembles: Optional[List[str]] = None,\n",
    "    indicators: Optional[List[str]] = None,\n",
    "    models: Optional[List[str]] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Filters files matching the pattern:\n",
    "    {model}_{scenario}-{ensemble}_{indicator}_ipcc-regions_latweight.csv\n",
    "\n",
    "    Args:\n",
    "        files: List of filenames to filter.\n",
    "        scenarios: List of scenarios to filter (e.g., ['ssp370', 'ssp245'])\n",
    "        ensembles: List of ensembles to filter (e.g., ['r4i1p1f1', 'r1i1p1f1'])\n",
    "        indicators: List of indicators to filter (e.g., ['pr', 'tas'])\n",
    "        models: List of models to filter (e.g., ['access-cm2', 'cesm2'])\n",
    "\n",
    "    Returns:\n",
    "        List of matching filenames.\n",
    "    \"\"\"\n",
    "    pattern = r'^(.+?)_(.+?)_(.+?)_ipcc-regions_latweight\\.csv$'\n",
    "    matching_files = []\n",
    "\n",
    "    for filename in files:\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            model, scenario_ensemble, indicator = match.groups()\n",
    "            scenario = '-'.join(scenario_ensemble.split('-')[0:-1])\n",
    "            ensemble = scenario_ensemble.split('-')[-1]\n",
    "            if scenarios and scenario not in scenarios:\n",
    "                continue\n",
    "            if ensembles and ensemble not in ensembles:\n",
    "                continue\n",
    "            if indicators and indicator not in indicators:\n",
    "                continue\n",
    "            if models and model not in models:\n",
    "                continue\n",
    "\n",
    "            matching_files.append(filename)\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "def get_baseline_filename(filename: str, files: List[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Finds the baseline file for a given scenario file.\n",
    "    - For SSP scenarios: returns historical file with same ensemble and indicator\n",
    "    - For non-SSP scenarios: returns piControl file with same ensemble and indicator\n",
    "    - If no exact match exists for non-SSP: returns piControl with different ensemble but same indicator\n",
    "\n",
    "    Args:\n",
    "        filename: The scenario filename\n",
    "        files: List of available filenames to search from\n",
    "\n",
    "    Returns:\n",
    "        The baseline filename if found, None otherwise\n",
    "    \"\"\"\n",
    "    # Parse the input filename\n",
    "    components = parse_filename(filename)\n",
    "\n",
    "    if not components:\n",
    "        print(f\"Error: Could not parse filename '{filename}'\")\n",
    "        return None\n",
    "\n",
    "    # Determine baseline scenario\n",
    "    if 'ssp' in components['scenario'].lower():\n",
    "        baseline_scenario = 'historical'\n",
    "    else:\n",
    "        baseline_scenario = 'picontrol'\n",
    "\n",
    "    # Construct expected baseline filename (same ensemble & indicator)\n",
    "    baseline_filename = (\n",
    "        f\"{components['model']}_{baseline_scenario}-{components['ensemble']}_\"\n",
    "        f\"{components['indicator']}_ipcc-regions_latweight.csv\"\n",
    "    )\n",
    "\n",
    "    # Check if exact baseline exists in provided files list\n",
    "    if baseline_filename in files:\n",
    "        return baseline_filename\n",
    "\n",
    "    # If not found and baseline is piControl, try fuzzy match (other ensemble)\n",
    "    if baseline_scenario == 'picontrol':\n",
    "        pattern = rf'^{re.escape(components[\"model\"])}_picontrol-(.+?)_' \\\n",
    "                  rf'{re.escape(components[\"indicator\"])}_ipcc-regions_latweight\\.csv$'\n",
    "\n",
    "        for file in files:\n",
    "            if re.match(pattern, file):\n",
    "                print(f\"Info: Exact ensemble match not found. Using '{file}' with different ensemble\")\n",
    "                return file\n",
    "\n",
    "        print(f\"Warning: No piControl baseline file found for indicator '{components['indicator']}'\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Warning: Baseline file '{baseline_filename}' not found in file list\")\n",
    "        return None \n",
    "    \n",
    "def reorder_columns(df):\n",
    "    # Ensure required columns exist\n",
    "    if 'time' not in df.columns or 'GLOBAL' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'time' and 'GLOBAL' columns\")\n",
    "\n",
    "    # Extract all columns except time and GLOBAL\n",
    "    middle_cols = [col for col in df.columns if col not in ['time', 'GLOBAL']]\n",
    "    \n",
    "    # Sort the region columns alphabetically\n",
    "    middle_cols_sorted = sorted(middle_cols)\n",
    "\n",
    "    # Reconstruct column order\n",
    "    new_column_order = ['time'] + middle_cols_sorted + ['GLOBAL']\n",
    "\n",
    "    # Reorder dataframe\n",
    "    return df[new_column_order]\n",
    "\n",
    "\n",
    "def process_scenarios(experiment_scenario_path, simulation_name, baseline_scenario_path = None, delete_first_years = 0, monthly_trend = False, smoothed = True):\n",
    "    \"\"\"\n",
    "    Load CMIP6 baseline (piControl) and abrupt4xco2 scenario data,\n",
    "    compute anomalies relative to the baseline scenario,\n",
    "    and return:\n",
    "        (1) global anomaly timeseries (annual, 21-year rolling mean)\n",
    "        (2) regional anomaly timeseries (monthly, 21-year rolling mean)\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "    df_experiment = pd.read_csv(experiment_scenario_path, parse_dates=[\"time\"])\n",
    "    df_experiment = reorder_columns(df_experiment)\n",
    "    df_experiment['time'] = df_experiment['time'].astype(str)\n",
    "    df_experiment['time'] = df_experiment['time'].apply(lambda x: datetime.strptime(x.split('.')[0], \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    #if 'historical' in baseline_scenario_path:\n",
    "    df_baseline = pd.read_csv(baseline_scenario_path, parse_dates=[\"time\"])\n",
    "    df_baseline = reorder_columns(df_baseline)\n",
    "    df_baseline['time'] = df_baseline['time'].astype(str)\n",
    "    df_baseline['time'] = df_baseline['time'].apply(lambda x: datetime.strptime(x.split('.')[0], \"%Y-%m-%d %H:%M:%S\"))\n",
    "  \n",
    "\n",
    "    first_year_experiment = df_experiment['time'][0].year\n",
    "    \n",
    "    if first_year_experiment < 1000: \n",
    "        first_year_experiment_shift = 1850 - first_year_experiment\n",
    "        df_experiment.time = df_experiment.time.map(lambda dt: dt.replace(year=dt.year + first_year_experiment_shift))\n",
    "        first_year_experiment = df_experiment['time'][0].year\n",
    "\n",
    "    \n",
    "    last_year_baseline = df_baseline['time'].iloc[-1].year\n",
    "    year_shift = (first_year_experiment - 1)  - int(last_year_baseline)\n",
    "    df_baseline.time = df_baseline.time.map(lambda dt: dt.replace(year=dt.year + year_shift))\n",
    "    #df_baseline['time'] = pd.to_datetime(df_experiment['time'])\n",
    "    \n",
    "    df_experiment = pd.concat([df_baseline, df_experiment]).sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    if (df_experiment['time'].iloc[-1].year - df_experiment['time'][0].year) > 450: \n",
    "        delete_additional_years = (df_experiment['time'].iloc[-1].year - df_experiment['time'][0].year) - 450\n",
    "        df_experiment = df_experiment[\n",
    "            [dt.year >= min(dt.year for dt in df_experiment['time']) + delete_additional_years for dt in df_experiment['time']]\n",
    "                ].reset_index(drop=True)\n",
    "\n",
    "        # Shift remaining years so first year becomes start_year\n",
    "        year_shift = 1750 -  df_experiment['time'][0].year\n",
    "        df_experiment['time'] = df_experiment['time'].apply(lambda dt: dt.replace(year=dt.year + year_shift))\n",
    "\n",
    "    #df_experiment['time'] = df_experiment['time'].apply(\n",
    "    #lambda x: f'{str(int(x[0])+1)}{x[1:]}' if int(x[0:3]) < 1500 else x\n",
    "    #)\n",
    "    #df_experiment['time'] = pd.to_datetime(df_experiment['time'])\n",
    "\n",
    "    first_year = df_experiment['time'][0].year\n",
    "    year_shift = 1750 - int(first_year)\n",
    "    df_experiment.time = df_experiment.time.map(lambda dt: dt.replace(year=dt.year + year_shift))\n",
    "    df_experiment['time'] = pd.to_datetime(df_experiment['time'])\n",
    "    \n",
    "    \n",
    "    region_cols = [col for col in df_experiment.columns if col.lower() != 'time']\n",
    "    \n",
    "    # calculate baseline\n",
    "    #if 'historical' not in baseline_scenario_path:\n",
    "    #    df_baseline = pd.read_csv(baseline_scenario_path, parse_dates=[\"time\"])\n",
    "    #    df_baseline = df_baseline.set_index(\"time\")\n",
    "    #    baseline_means = df_baseline[region_cols].mean()\n",
    "    #else: \n",
    "    df_baseline = copy.deepcopy(df_experiment)\n",
    "    df_baseline['time'] = df_baseline['time'].astype(str)\n",
    "    df_baseline['time'] = df_baseline['time'].apply(lambda x: datetime.strptime(x.split('.')[0], \"%Y-%m-%d %H:%M:%S\"))\n",
    "    #df_baseline['time'] = pd.to_datetime(df_baseline['time'])\n",
    "    df_baseline = df_baseline[(df_baseline['time'].dt.year >= 1850) & (df_baseline['time'].dt.year <= 1900)]\n",
    "    baseline_means = df_baseline[region_cols].mean()\n",
    "\n",
    "    if delete_first_years != 0: \n",
    "        df_experiment = df_experiment[df_experiment['time'].dt.year >= df_experiment['time'].dt.year.min() + delete_first_years].reset_index(drop=True)\n",
    "        # Shift remaining years so first year becomes start_year\n",
    "        year_shift = 1850 - df_experiment['time'].dt.year.min()\n",
    "        df_experiment['time'] = df_experiment['time'].apply(lambda dt: dt.replace(year=dt.year + year_shift))\n",
    "\n",
    "    df_experiment = df_experiment.set_index(\"time\")\n",
    "\n",
    "    # (B) Compute regional anomalies and apply 21-year (252-month) rolling mean\n",
    "    df_regional_anomaly = df_experiment[region_cols] - baseline_means\n",
    "    #df_regional_smoothed = df_regional_anomaly\n",
    "    if monthly_trend:\n",
    "        if smoothed:\n",
    "            df_regional_smoothed = (\n",
    "                df_regional_anomaly\n",
    "                .groupby(df_regional_anomaly.index.month)\n",
    "                .apply(lambda x: x.rolling(window=21, center=True).mean())\n",
    "                .reset_index(level=0, drop=True))  \n",
    "        else: \n",
    "            df_regional_smoothed = (\n",
    "                df_regional_anomaly\n",
    "                .groupby(df_regional_anomaly.index.month)\n",
    "                .apply(lambda x: x)  # just keep values as they are\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "        df_global = pd.DataFrame({\n",
    "        'time': df_experiment.index,\n",
    "        'GMT': df_regional_smoothed.GLOBAL#df_regional_smoothed.mean(axis=1)\n",
    "            }).set_index('time')\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        #df_regional_smoothed = df_regional_anomaly.rolling(window=21*12, center = True).mean()\n",
    "        df_annual = df_regional_anomaly.resample('Y').mean()\n",
    "        if smoothed:\n",
    "            df_regional_smoothed = df_annual.rolling(window=21, center=True).mean()\n",
    "        else: \n",
    "            df_regional_smoothed = df_annual.copy()\n",
    "        \n",
    "        df_global = pd.DataFrame({\n",
    "        'time': df_regional_smoothed.index,\n",
    "        'GMT': df_regional_smoothed.GLOBAL#df_regional_smoothed.mean(axis=1)\n",
    "    }).set_index('time')\n",
    "    \n",
    "   \n",
    "    # Convert monthly to annual mean\n",
    "    df_global['year'] = df_global.index.astype(str).str[:4].astype(int)\n",
    "    df_global_annual = df_global.groupby('year')['GMT'].mean().reset_index()\n",
    "    #df_global_annual = df_global_annual.set_index(\"year\")\n",
    "    \n",
    "    # Apply 21-year annual rolling mean\n",
    "    #print(df_global_annual['GMT'])\n",
    "    #df_global_annual['GMT'] = df_global_annual['GMT'].rolling(window=21, center = True).mean()\n",
    "\n",
    "    # Add simulation name\n",
    "    df_global_annual.insert(0, 'simulation_name', simulation_name)\n",
    "    \n",
    "    # Remove GMT from regional temperature timeseries\n",
    "    \n",
    "    df_regional_smoothed.drop(columns=['GLOBAL'], inplace = True)\n",
    "   \n",
    "    return df_global_annual, df_regional_smoothed\n",
    "\n",
    "def detect_is_monthly(df, gmt_df):\n",
    "    idx = df.index\n",
    "\n",
    "    # Case 1: Index is datetime → use infer_freq\n",
    "    if isinstance(idx, pd.DatetimeIndex):\n",
    "        freq = pd.infer_freq(idx)\n",
    "        return freq in ['M', 'MS']\n",
    "\n",
    "    # Case 2: Index is integer years → assume annual\n",
    "    if np.issubdtype(idx.dtype, np.integer):\n",
    "        # Annual data should have roughly same length as GMT series\n",
    "        return len(df) > len(gmt_df)\n",
    "\n",
    "    # Case 3: Index is string like '2000-01', '1999-12'\n",
    "    try:\n",
    "        idx_dt = pd.to_datetime(idx, format='%Y-%m')\n",
    "        freq = pd.infer_freq(idx_dt)\n",
    "        return freq in ['M', 'MS']\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Default fallback\n",
    "    return False\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def expand_annual_to_monthly(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Take annual data indexed by year (DatetimeIndex or year ints)\n",
    "    and return monthly data via linear interpolation.\n",
    "    Ensures first = Jan, last = Dec.\n",
    "    \"\"\"\n",
    "\n",
    "    s = series.copy()\n",
    "\n",
    "    # ---- 1. Make the index timezone-naive and normalized ----\n",
    "    if isinstance(s.index, pd.DatetimeIndex):\n",
    "\n",
    "        # If tz-aware → remove timezone\n",
    "        if s.index.tz is not None:\n",
    "            idx = s.index.tz_convert(None)\n",
    "        else:\n",
    "            idx = s.index  # already tz-naive\n",
    "\n",
    "        # Normalize (remove hour/min/sec)\n",
    "        s.index = idx.normalize()\n",
    "\n",
    "    else:\n",
    "        # e.g. Int64Index of years → convert to Timestamp\n",
    "        s.index = pd.to_datetime(s.index.astype(str) + \"-01-01\")\n",
    "\n",
    "    # ---- 2. Add Dec-31 entry for last year ----\n",
    "    first_year = s.index.year.min()\n",
    "    last_year = s.index.year.max()\n",
    "\n",
    "    start = pd.Timestamp(f\"{first_year}-01-01\")\n",
    "    end = pd.Timestamp(f\"{last_year}-12-01\")\n",
    "\n",
    "    # Ensure last year has a December timestamp\n",
    "    if end not in s.index:\n",
    "        s.loc[end] = s.loc[pd.Timestamp(f\"{last_year}-01-01\")]\n",
    "        s = s.drop(pd.Timestamp(f\"{last_year}-01-01\"))\n",
    "    # ---- 3. Create complete monthly index ----\n",
    "    full_monthly_index = pd.date_range(start=start, end=end, freq=\"MS\")\n",
    "\n",
    "    # ---- 4. Reindex & interpolate monthly ----\n",
    "    s = s.reindex(full_monthly_index).interpolate(\"linear\")\n",
    "\n",
    "    return s\n",
    "\n",
    "def predict_regional_temperatures(global_test_series, slopes, intercepts, same_shape = False):\n",
    "    \"\"\"\n",
    "    Uses stored regression parameters to predict regional temps.\n",
    "    Returns:\n",
    "        predictions: array of shape (len(global_test_series), n_regions)\n",
    "    \"\"\"\n",
    "    original_shape = np.asarray(global_test_series).shape\n",
    "    global_test_series = np.asarray(global_test_series).reshape(-1, 1)\n",
    "    predictions = global_test_series * slopes + intercepts\n",
    "\n",
    "    if same_shape:\n",
    "        # Collapse regional dimension (e.g., mean or first region)\n",
    "        # Here: return full region predictions but reshaped like input\n",
    "        # so we return an array where the last dimension = n_regions.\n",
    "        return predictions.reshape(*original_shape, -1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def fit_regional_regressions(global_series, regional_series):\n",
    "    \"\"\"\n",
    "    Fits one linear regression per region: region = a + b * global\n",
    "    Returns:\n",
    "        slopes:   (46,) array of regression coefficients\n",
    "        intercepts: (46,) array of intercepts\n",
    "    \"\"\"\n",
    "    global_series = np.asarray(global_series).reshape(-1, 1)\n",
    "    regional_series = np.asarray(regional_series)\n",
    "    \n",
    "    n_regions = regional_series.shape[1]\n",
    "    slopes = np.zeros(n_regions)\n",
    "    intercepts = np.zeros(n_regions)\n",
    "\n",
    "    for i in range(n_regions):\n",
    "        model = LinearRegression()\n",
    "        model.fit(global_series, regional_series[:, i])\n",
    "        slopes[i] = model.coef_[0]\n",
    "        intercepts[i] = model.intercept_\n",
    "\n",
    "    return slopes, intercepts\n",
    "\n",
    "def process_gmt_and_regions_into_array(data_tuple: Tuple[pd.DataFrame, pd.DataFrame], weighted_linear_smoothing = False, pattern_scaling_residuals = False, slope_intercept = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Processes a single tuple of (GMT DataFrame, regional DataFrame) into\n",
    "    a numpy array of shape (1 + number_regions) x number_timesteps.\n",
    "\n",
    "    Steps:\n",
    "    1. Remove NaN values\n",
    "    2. If regional data is monthly -> interpolate GMT to monthly\n",
    "       If regional data is annual -> keep GMT as is\n",
    "    3. Align data by time\n",
    "    4. Convert to numpy array: first row GMT, rest regional\n",
    "    \"\"\"\n",
    "    gmt_df, region_df = data_tuple\n",
    "\n",
    "    # --- 1. Clean NaN values ---\n",
    "    gmt_df = gmt_df.dropna(subset=['GMT'])\n",
    "    region_df = region_df.dropna(axis=0, how='all')  # Remove rows where all regions are NaN\n",
    "    # --- 2. Determine if regional data is monthly or annual ---\n",
    "    #freq = pd.infer_freq(region_df.index)\n",
    "    is_monthly = detect_is_monthly(region_df, gmt_df)#len(region_df.index) > len(gmt_df) or freq in ['M', 'MS'] # detect_is_monthly(region_df, gmt_df)\n",
    "\n",
    "    # --- 3. Prepare GMT time series ---\n",
    "    # Convert year to datetime for consistency\n",
    "    gmt_df['time'] = pd.to_datetime(gmt_df['year'], format='%Y')\n",
    "    gmt_ts = gmt_df.set_index('time')['GMT']\n",
    "    \n",
    "\n",
    "    if is_monthly:\n",
    "        # Create monthly index from the regional dataframe\n",
    "        monthly_index = region_df.index\n",
    "\n",
    "        # Interpolate GMT to monthly\n",
    "        gmt_monthly = expand_annual_to_monthly(gmt_ts)#gmt_ts.reindex(\n",
    "            #pd.date_range(gmt_ts.index.min(), gmt_ts.index.max(), freq='MS')\n",
    "        #).interpolate('linear')\n",
    "    \n",
    "        # Align GMT to regional timestamps (forward/backward fill allowed)\n",
    "        gmt_aligned = gmt_monthly#.reindex(monthly_index)\n",
    "    else:\n",
    "        # Assume annual data, map directly by year\n",
    "        region_df = region_df.copy()\n",
    "        region_df['year'] = region_df.index.year\n",
    "        gmt_aligned = region_df['year'].map(dict(zip(gmt_df['year'], gmt_df['GMT']))) \n",
    "\n",
    "\n",
    "    # --- 4. Merge GMT and regional data ---\n",
    "    # Ensure no NaN in GMT after alignment\n",
    "    gmt_vals = np.array(gmt_aligned.dropna()).reshape(1, -1)\n",
    "  \n",
    "    # Drop non-regional columns and convert to numpy\n",
    "    region_clean = region_df.drop(columns=[col for col in ['year'] if col in region_df.columns])\n",
    "    regional_vals = region_clean.T.values\n",
    "    \n",
    "    gmt_vals = gmt_vals[:,:regional_vals.shape[1]]\n",
    "    # --- 5. Combine GMT + Regional into final array ---\n",
    "    result_array = np.vstack([gmt_vals, regional_vals])\n",
    "\n",
    "    if weighted_linear_smoothing:\n",
    "        return smooth_regional_indicator_timeseries(regional_indicator=result_array, bandwidth=20, is_monthly=is_monthly)\n",
    "    \n",
    "    if pattern_scaling_residuals:\n",
    "        if slope_intercept == None: \n",
    "            slopes, intercepts = fit_regional_regressions(gmt_vals, regional_vals.transpose(1,0))\n",
    "            return slopes, intercepts\n",
    "        else: \n",
    "            slopes, intercepts = slope_intercept\n",
    "            gmt_vals_preds = copy.deepcopy(gmt_vals)\n",
    "            ps_prediction = np.squeeze(predict_regional_temperatures(gmt_vals_preds, slopes, intercepts, same_shape = True)).T\n",
    "            \n",
    "            regional_vals = ps_prediction - regional_vals\n",
    "            \n",
    "            # (58, 231)\n",
    "            # (58, 231)\n",
    "            # (58, 231)\n",
    "            # (1, 231)\n",
    "            result_array_residuals = np.vstack([gmt_vals, regional_vals])\n",
    "            return result_array_residuals\n",
    "    \n",
    "    return result_array\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prepare_train_data(data,n):\n",
    "    \"\"\" \n",
    "    data: array with shape (1 + number_regions, number_timesteps)\n",
    "    n: window size length\n",
    "    \"\"\"\n",
    "    regions, timesteps = data.shape\n",
    "    first_region = data[0]   # shape: (timesteps,)\n",
    "    other_regions = data[1:] # shape: (number_regions, timesteps)\n",
    "\n",
    "    # Number of valid training windows\n",
    "    num_samples = timesteps - n - 1\n",
    "\n",
    "    # X shape target: (regions, n, num_samples)\n",
    "    X = np.zeros((regions, n, num_samples))\n",
    "\n",
    "    for i in range(num_samples):  # x goes from n to timesteps-1\n",
    "        # First region: t[x-n] ... t[x]  (length n)\n",
    "        X[0, :, i] = first_region[(i+1):(i+n+1)]\n",
    "\n",
    "        # Other regions: t[x-n-1] ... t[x-1] (also length n)\n",
    "        X[1:, :, i] = other_regions[:, i:(i+n)]\n",
    "\n",
    "    # Y shape target: (number_regions, num_samples)\n",
    "    Y = np.zeros((regions - 1, num_samples))\n",
    "\n",
    "    # At time t[x], take all region values except the first one\n",
    "    Y[:, :] = other_regions[:, (n+1):]  # n to end: timesteps - n samples\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def prepare_all_train_data(data_arrays, n):\n",
    "    \"\"\"\n",
    "    data_arrays: list of arrays, each shape (1 + number_regions, number_timesteps)\n",
    "    n: window size length\n",
    "    \"\"\"\n",
    "\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "\n",
    "    for data in data_arrays:\n",
    "        \n",
    "        X, Y = prepare_train_data(data,n)\n",
    "\n",
    "        X_list.append(X)\n",
    "        Y_list.append(Y)\n",
    "\n",
    "    # Combine multiple samples along the third dimension\n",
    "    X_combined = np.concatenate(X_list, axis=2)  \n",
    "    Y_combined = np.concatenate(Y_list, axis=1)\n",
    "\n",
    "    return X_combined, Y_combined\n",
    "\n",
    "def shuffle_train_data(X, Y, random_state=None):\n",
    "    \"\"\"\n",
    "    Shufflestraining data together so their sample alignment stays correct.\n",
    "\n",
    "    X shape: (features, n, samples)\n",
    "    Y shape: (targets, samples)\n",
    "\n",
    "    Returns: shuffled X and Y\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    num_samples = X.shape[2]\n",
    "    indices = np.random.permutation(num_samples)\n",
    "\n",
    "    X_shuffled = X[:, :, indices]   # shuffle along last axis\n",
    "    Y_shuffled = Y[:, indices]      # shuffle along last axis\n",
    "\n",
    "    return X_shuffled, Y_shuffled\n",
    "\n",
    "\n",
    "potential_files = get_all_files(MODEL_PATH)\n",
    "\n",
    "train_files = filter_climate_files(files = potential_files, scenarios = TRAIN_SCENARIOS, indicators = [INDICATOR])\n",
    "test_files = filter_climate_files(files = potential_files, scenarios = TEST_SCENARIOS, indicators = [INDICATOR])\n",
    "\n",
    "train_files_with_baseline = [(get_baseline_filename(filename=filename, files= potential_files), filename) for filename in train_files]\n",
    "test_files_with_baseline = [(get_baseline_filename(filename=filename, files= potential_files), filename) for filename in test_files]\n",
    "\n",
    "for (base,exp) in test_files_with_baseline: \n",
    "    print(base, exp)\n",
    "\n",
    "train_data_df = [process_scenarios(experiment_scenario_path = f'{MODEL_PATH}/{experiment}', simulation_name = experiment, baseline_scenario_path = f'{MODEL_PATH}/{baseline}', delete_first_years = 0, monthly_trend = False, smoothed = True) for (baseline,experiment) in train_files_with_baseline]\n",
    "test_data_df = [process_scenarios(experiment_scenario_path = f'{MODEL_PATH}/{experiment}', simulation_name = experiment, baseline_scenario_path = f'{MODEL_PATH}/{baseline}', delete_first_years = 0, monthly_trend = False, smoothed = True) for (baseline,experiment) in test_files_with_baseline]\n",
    "\n",
    "train_data_df = [process_scenarios(experiment_scenario_path = f'{MODEL_PATH}/{experiment}', simulation_name = experiment, baseline_scenario_path = f'{MODEL_PATH}/{baseline}', delete_first_years = 0, monthly_trend = False, smoothed = True) for (baseline,experiment) in train_files_with_baseline]\n",
    "test_data_df = [process_scenarios(experiment_scenario_path = f'{MODEL_PATH}/{experiment}', simulation_name = experiment, baseline_scenario_path = f'{MODEL_PATH}/{baseline}', delete_first_years = 0, monthly_trend = False, smoothed = True) for (baseline,experiment) in test_files_with_baseline]\n",
    "\n",
    "if PATTERN_SCALING_RESIDUALS:\n",
    "    ssp585_index = [i for i, f in enumerate(train_files) if 'ssp585' in f][0]\n",
    "    slope, intercept = process_gmt_and_regions_into_array(train_data_df[ssp585_index], weighted_linear_smoothing = False, pattern_scaling_residuals=PATTERN_SCALING_RESIDUALS)\n",
    "    train_data_np = [process_gmt_and_regions_into_array(GMT_regional_values_tuple, weighted_linear_smoothing = False, pattern_scaling_residuals=PATTERN_SCALING_RESIDUALS, slope_intercept = (slope, intercept)) for GMT_regional_values_tuple in train_data_df]\n",
    "    test_data_np = [process_gmt_and_regions_into_array(GMT_regional_values_tuple, weighted_linear_smoothing = False, pattern_scaling_residuals=PATTERN_SCALING_RESIDUALS, slope_intercept = (slope, intercept)) for GMT_regional_values_tuple in test_data_df]\n",
    "else: \n",
    "    train_data_np = [process_gmt_and_regions_into_array(GMT_regional_values_tuple, weighted_linear_smoothing = False) for GMT_regional_values_tuple in train_data_df]\n",
    "    test_data_np = [process_gmt_and_regions_into_array(GMT_regional_values_tuple, weighted_linear_smoothing = False) for GMT_regional_values_tuple in test_data_df]\n",
    "\n",
    "train_data_input, train_data_output = prepare_all_train_data(train_data_np, n=N)\n",
    "test_data_input, test_data_output = prepare_all_train_data(test_data_np, n=N)\n",
    "\n",
    "train_data_shuffeled = shuffle_train_data(train_data_input, train_data_output, random_state=42)\n",
    "\n",
    "test_data_for_autoregression_input, test_data_for_autoregression_output = prepare_train_data(test_data_np[0],N)\n",
    "\n",
    "gmt_autoregressive_test = test_data_np[0][0,:]\n",
    "autoregressive_test_groudtruth = test_data_np[0][1:,:]\n",
    "\n",
    "gmt_train = train_data_np[0][0,:]\n",
    "train_data_regional_temperature = train_data_np[0][1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 431)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6NklEQVR4nO3df1yUdb7//+cMAgoKiCZgYlJaHUL8lRq51aaYJvnRrbO3rPbU6ddum7aa+902tx9+O9WHTn3atdKy7Nc5pzXbtl+f1GwJzR+Fkj8IkWrVME0BMxQUFXHm+vxhEBgw1wwzc83M9bjfbnO75cz7Gt7jVc2T6/16vy6HYRiGAAAALOK0egIAAMDeCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEt1sXoCZrjdbu3bt089evSQw+GwejoAAMAEwzB0+PBh9e3bV05n+9c/wiKM7Nu3T+np6VZPAwAA+GDPnj3q169fu6+HRRjp0aOHpFMfJiEhweLZAAAAM+rq6pSent78Pd6esAgjTUszCQkJhBEAAMKMpxILClgBAIClCCMAAMBShBEAAGApwggAALAUYQQAAFiKMAIAACxFGAEAAJYijAAAAEuFRdMzAKHP5TZUXFGj/YePq0+PrhqVkawoJ/eSAuAZYQSA11oGj97xsdpQ8b1e/qRCRxpczWOS46P1yJQsTcrua+FMAYQDwggArywvrdT975Wppv5Eh+Nq6ht15+Itun3PQd2Xd4GkUyFm/c7v9cnO77T34DFJp9pEn9mzmy4+p7cuOrsXV1MAGyKMADDt0WXlWrS2wqtjFq3dpZNuQ0ndYvX8mp06esLV5rgFq3YqtotDv73sHN017lxCCWAjDsMwDKsn4UldXZ0SExNVW1vLjfIAizy8dJteWrcrKD8ryiGNz0zRv+UM4GoJEMbMfn9zZQSAR48uC14QkSSXIa3YVq0V26qVFBetx64erIlZaUH7+QCCizACoEPLS/dp0dpdlv38Q0cbdcdrm3X1sDSlJHbTvoPHqDMBIgxhBEC7XG5D979XZvU0JElvb6n8yXMLVu3kygkQAbxqepafn6+RI0eqR48e6tOnj6ZOnaqvvvrK43Fvvvmmzj//fHXt2lWDBw/W8uXLfZ4wgOAprqhRTX2j1dPoUNOVkxVlPw0rAMKDV2Fk9erVmj59utavX6+CggI1NjbqiiuuUH19fbvHfPrpp7ruuut06623asuWLZo6daqmTp2qsrLQ+G0LQPs+Kq+yegqm/fGtUrncIV+PD6ANndpN891336lPnz5avXq1Lr300jbHXHvttaqvr9fSpUubn7vooos0dOhQLVy40NTPYTcNEHwut6GRjxaE/JWRlq7KTtP864dbPQ0APzD7/d2pe9PU1tZKkpKTk9sdU1RUpNzc3FbPTZgwQUVFRe0e09DQoLq6ulYPAMHlyxKN1WWkS0srtbRkn8WzAOAtn8OI2+3WrFmzNGbMGGVlZbU7rqqqSikpKa2eS0lJUVVV+5d/8/PzlZiY2PxIT0/3dZoAfLT/8HFT485K7qYZlw/UX28brR3/e5Ju/dkAU8d1cUgXnpWkKD/frvN3b2zR8lLqR4Bw4vNumunTp6usrEzr1q3z53wkSXPmzNHs2bOb/1xXV0cgAYKsT4+upsY9ds0Q5ZzTq/nPD1x1gZwOh8dOrfNvGK6JWWlyuQ09U7hdz328Qw2uztd8uA3pzsWbtdA5nB02QJjw6XeSGTNmaOnSpVq1apX69evX4djU1FRVV1e3eq66ulqpqantHhMbG6uEhIRWDwDBdbC+weOYtMRTd+c93X15mXr2+uHqHvvT33d6xkVr4a9+DApRTodmjT9X5Q9fqVnjBnV+4j+49+2tFLQCYcKrKyOGYeiuu+7SO++8o48//lgZGRkej8nJyVFhYaFmzZrV/FxBQYFycnK8niyA4HC5DT287AuP4x7Iy2y34dik7DRNyErV+p3fq+jrA5IcyjmnV7tNyppCyflpPXTv21t16GjnCmcPHW3U/JU7NDPXfwEHQGB4FUamT5+uxYsX67333lOPHj2a6z4SExPVrVs3SdKNN96oM888U/n5+ZKkmTNn6rLLLtOTTz6pvLw8LVmyRBs3btQLL7zg548CwF+KK2pUWeu5ZqRnfEyHr0c5HRozqLfGDOpt+mdPzErT+MxTIea/1+/S6n9+p+ONbtPHt/TKpxWaMXYgHVqBEOdVGHnuueckST//+c9bPf/KK6/o3//93yVJu3fvltP54+rPxRdfrMWLF+v+++/Xn/70Jw0aNEjvvvtuh0WvAKxltnjV7DhvtQwxLreh4ooa7T98XH16dNXKL6tN3zn40NFGFVfUtKppARB6vF6m8eTjjz/+yXO//OUv9ctf/tKbHwXAQrsOtN/IsCWzRa6dEeV0tAoTOef00pAzkzRjyRZTxxeUVxFGgBDn5011AMKdy23o9eLdHse1V7waDFcN7Wu62PWNz/ZQyAqEOMIIgFaKK2pUVed5J820kf0trcW4a9wgxcdGeRxXf8KlmSavogCwBmEEQCtm70czoHdcgGfSsSinQ9MuNNd/aGlpJY3QgBBGGAHQzOU29E7JXlNjg1Ev4kluZvv9ik73wHtlLNcAIYowAqCZ2fvR9IqPsaxepKVRGclK6hZtauz39SdUXFET4BkB8AVhBEAzs1t1pwztGxK9O6KcDt08ZoDp8QUml6AABBdhBEAzs1t6x3uxPBJoM8aaK2SVpL9t/JalGiAEEUYASDpVL7J4Q2hv6W1LlNOhJ67JNjX2SMNJzV+5I8AzAuAtwggASdL8ldtVfTj0t/S2ZVJ2X407/wxTYxeu3sHVESDEEEYAaEVZpf7y0XZTY63e0tue2y45x9S4Y41u3bCoiEAChBDCCGBzLrehe9/eanp8KGzpbYs3O2vWVxzUBQ+u0PLSfQGeFQAzCCOAzc1fuV2HjnreziuFXr1IS97urDl+0q07F2/Ro8u2BW5SAEwhjAA25nIbeuWTXabHz52cGXL1Ii3NGDtIcTHmdtY0WbR2lx5eSiABrEQYAWysuKJGh46Zuypyd+65mpiVFuAZdU6U06HfXHq218e9tI5AAliJMALYmNkmZ4ndumjG2IEBno1/eNN3pCUCCWAdwghgY2aLUW8Zc3ZIL8+05E3fkdMRSABrdLF6AgCsc7Dec1+RpLjosLkq0mRSdl/dvuegFq3d5fWxL63bpY3f1OiSgX2Uc04vjRyQrE3fHNT+w8fVp8epAt5wCWZAuCCMADblcht6eNkXHsf976mDw/LL9768CyQ5tGhthdfHfr6nTp/vqdP8VT/t1pocH61HpmRpUnZfP8wSgMQyDWBbxRU1qqz1XDPSMz4mCLMJjPvyMvXs9cMVG+W/MFVT36g7F2/R9L9upHEa4CeEEcCmzBavmh0XqiZlp6n84SuVl+Xfm/st21qtc+9brjv+Z6M+2XGAYAJ0AmEEsCmzxauh2nHVG1FOhxb8aoRu/dkAv76vy5BWbKvWDS9uUPZDH9LRFfARYQSwKTPFq6HccdUXD1x1gd8DSZP6BhcdXQEfEUYAGzJbvPpAXmh3XPVFIAOJREdXwBeEEcCG7FC82pFABxL6lQDeIYwANmSX4tWOEEiA0EEYAWzITsWrHXngqgt0+yUZAXv/l9bt0qPLygP2/kCkIIwANjQqI1lJcdHtvu5Q5BWvtqepF0lygJakFq2t0PLSyoC8NxAp6MAK2FBBeZUOHW3/br2GpLmTI694tT2TstM0IStVxRU1zW3fR5zVU59V1Kjo6wPavv+IVn65X40u33qJ3PNWqSZkpdrm7xPwFmEEsBmX29BD73e8dJAUF63xmf5tEhbqopwO5ZzTq9VzYwb11phBvSWd+nt7pnC7Fq7eqeMn3V6995GGk5q/codm5g7y23yBSMIyDWAzZnbSHDraqOKKmiDNKDxEOR2aNf5cbfuPifrrraM1pF+CV8cvXL2DLq1AOwgjgM2wk6ZzopwOjRnUW+/NuMSr3TjHGt2auWRL4CYGhDHCCGAz7KTxH2+3By8traSYFWgDYQSwGTu2gQ+kB666QHmDzdfX3PNWKcs1wGkII4CN2LkNfCA9fd1wxcVEmRrbVMwK4EeEEcBG7N4GPlCinA795tKzTY9/5dMKro4ALRBGABuheDVwZowdpPhYc1dH2K0EtEYYAWyE4tXAiXI69MQ12abHF5RXBXA2QHghjAA2Qhv4wJqU3VeTs80Vs75Xso+lGuAHhBHARmgDH3jzpg03tVzzff0JlmqAHxBGAJugDXxwRDkdmnZhuqmxLNUApxBGAJugDXzw5JoMdCzVAKcQRgCbYCdN8IzKSFZyfPu1OU1YqgFOIYwANsFOmuCJcjr0i6FnmhrLUg1AGAFsgzbwwWV2qebdLXtZqoHtEUYAG6ANfPCZXaqpOdpIe3jYHmEEsAHawAefN0s1f/non1pRxt18YV+EEcAGKF61htmlGkl66P1ylmtgW4QRwAYoXrXGqIxkpSWa+zutrD3OzhrYFmEEsAGKV60R5XRo7uRM0+PZWQO7IowAEY7iVWtNzErT3bmDTI2lCRrsijACRDiKV603Y+wg9ezgBoVNaIIGuyKMABGO4lXrRTkdunoYTdCA9hBGgAi360C9qXEUrwYW96sB2kcYASKYy23o9eLdHsdRvBp43K8GaB9hBIhgxRU1qqrzvJNm2sj+FK8GGPerAdpHGAEimNk6kAG94wI8E0gs1QDtIYwAEYxmZ6GFpRqgbYQRIILR7Cy0sFQDtI0wAkQomp2FJrNLNX/b+C1LNbANwggQoWh2FprMLtUcaTip+St3BGFGgPUII0CEotlZaPJmqeb5NTu5OgJbIIwAEYri1dBldqnm6AkXV0dgC4QRIEI13b6+vWoQhyhetcqojGQldfO8VCNJr3xawdURRDzCCBChopwO/a8haeroa2zuZIpXrRDldOjmMQNMjT10tJFtvoh4hBEgQq0oq9QLayraff3Xl2ZoYlZaEGeElmaMHaS4mChTYxet3Rng2QDWIowAEcjlNvTQ++UdXhX5v59XcvnfQlFOh35z6dmmxq788jstL60M8IwA63gdRtasWaPJkyerb9++cjgcevfddzsc//HHH8vhcPzkUVVFQx8gUMxs662sPc7lf4vNGDtI8bHmro7c81Yp4RF+5XIb+mT7Af2fD7/U//nwK32y44Bl/4518faA+vp6DRkyRLfccouuvvpq08d99dVXSkhIaP5znz59vP3RAExiW294iHI6NO3CdL30yS6PY5v6jszMHRT4iSHiLS+t1D1vlepIw8nm5+av2qGkuGg9dvXgoC/hen1l5Morr9QjjzyiX/ziF14d16dPH6WmpjY/nE5WiIBAYVtv+DC7zVeSFq7ewdURdNqjy8p15+LNrYJIk0NHG3XHa5u1oiy4y4JBSwRDhw5VWlqaxo8fr08++aTDsQ0NDaqrq2v1AGAe23rDh9mOrJJ0rNGtmUu2BHhGiGQPL92mRWvbL2xv8tD75UENvgEPI2lpaVq4cKHeeustvfXWW0pPT9fPf/5zbd68ud1j8vPzlZiY2PxIT08P9DSBiMK23vAR5XTokSlZpscvLa3UvIJ/coUkCE6vqVj7z+/0yY4Deq9kr4p2fh925+Dhpdv00rpdpsYGu6bMYRiGz3+bDodD77zzjqZOnerVcZdddpn69++v//mf/2nz9YaGBjU0/Hi30bq6OqWnp6u2trZV3QmAtq0oq9RvX9vcbhj5zaUZmjMpM6hzQsfuWrxJ75eaL+zvGRetR6dmaVJ23wDOyn5cbkPrd36v1zbs0qovv9Pxk+52xybHR+uRKeFxDrwJIk2emjZUU0zeuqA9dXV1SkxM9Pj9bUnhxqhRo7RjR/stjmNjY5WQkNDqAcActvWGp3nThpvuOyJJB4826s7FW5S/vDyAs7KX5aWVGvLQP3TDSxv0QVl1h0FEkmrqT52DR5dtC9IMfeNLEJGCW1NmSRgpKSlRWhrNloBAYFtvePKm70hLz6+poAeJH3RU1OnJorW79Oiy0AyFvgaRYNeUeb2198iRI62ualRUVKikpETJycnq37+/5syZo7179+q///u/JUnz5s1TRkaGLrjgAh0/flwvvviiVq5cqX/84x/++xQAmrGtN3zNGDtIL6z9WvUNLq+O+8PfP9eErFRqgHzk6xd2S4vWVmhYek9Nyrb2F+2mZaZPdn6nFWWV+vrAMZ/eJ9g1ZV6HkY0bN+ryyy9v/vPs2bMlSTfddJNeffVVVVZWavfu3c2vnzhxQr///e+1d+9excXFKTs7Wx999FGr9wDgP2zrDV9RToeeuCZbdy72bsdM/QmXfvf6Zi24YUSAZhZ5mr60H//wC33+rX92bN7zVqmlobCt3iHeio+N0pO/HBL0PiOdKmANFrMFMACk5aX7PH6ZpSV21bo/juU36RDlbTFrk9svydB9eRQme+KPL+323J17riWN6fKXl+v5Du5FZcakrBQ9c/0Iv/5/IaQLWAEEhstt6OFlX3gc90Ae23pD2bxpw023iW9p0VrqRzzpTG2IGc+v2Rn04vClJfs6HURu/dkAPfurCy37/4LXyzQAQpeZ4lVJ6hkfE4TZwFe+LtdI1i8VdFbLmoe9B3+sd3A4HDqzZzddfE5vXXR2L58+nz9qQzw5esIV1Lb9y0v36a5ONsK79WcD9MBVF/hpRr4hjAARhOLVyDEpu69+8+0hr3/jPdJwUvf8/XM9/q9Dwi6QmFk+WbBqp+Jjo/TENdle9fcIRhBp8sqnFZoxdmDA//5XlFX6FFhbCoUgIrFMA0QUilcjy5xJmXr2+uHqGWeuXXyTtzbvVeaDH2hewVdh00/Gm+WT+gaXV/09Hl0WvCAinbq/S6C3zrvchu59e2un3iNUgojElREgoozKSFZSXLQOHW1s83WHpFTuSRNWJmWnaUJWqp4u3K6nCrebPq7hpKF5hTu0YNVOTb/8HN017lxLr5S43IaKK2pUVXtMB440qOboCe37YRlm695DPm1BXbR2l9yGOvxCXVqyT4vW7vJ12hqenqh9tQ2qqvPuamJBeZVyzunl88/1ZNaSze3+d25GKAURiTACRJSC8qoO/wdliHvShKMop0N3jz9XR080ev3F2ug+FUrmr9yhKcP66meD+ig14VQgDda/B8tLK3X/e2WqqT/h9/d+ad0uOR2ONncRLS3ZpxmdqKdo+sJuClL7Dx/X2n8e0N83f+vx2L9t/Fb3BahQfHnpPp92WzW5/ZIBui8vdIKIxNZeIGK43IZ+9p8rOyxgTYqL1qb7xxNGwtj0v27Ssq2+fxE1SUvsqrmTMwPeT+LRZeWm7hLbWb+7fKBmjBukzypqVPT1Aa3bcUAle2p9fr/2vrBdbkPZD31oqjFdILb5evPzT9c9NkqPe1lr01lmv78JI0CEKNr5va5btN7juNdvvyigl48RWC63oRGPFHTqEn1LC381PGCBJJhFo/5i5gv74fe36aVPdnl8r27RTpU9NNGv4d/bHjQjz+qpUWcnd2oXUmfQZwSwGXbS2EOU06HHrh7st/f741ulASlyDbcgMrRfgv5622h9PneCxysHuZmppt7zWKNbMzu57bYlb5dnZo4bpDd/e7H+MOF8jRnYO6SviBJGgAjBThr7mJiVpmevHyZ/fLXUHjupGxYV+TWQBHv3Smfd+rMBenfGJaa/sEdlJCupm7kdTktLK7W0ZF9npyiX29Af3io1Pb57bBf9blzwO8H6ijACRIimnTTtcSj4d+JE4EzK7qsF1w/3y3utrzio7P//Q60o63z31uWlndu9Emx5g1O93lUS5XTo5jEDTI+fsWRLpwPJ/JXbvaoTefya7JC+EnI6wggQIdhJYz+TstO08FfDFRfjfev409WfcOmO1zZ3KpB4+9u7GWf36qaze3fz63s2SezWRU9f51ugmzF2kFd/7zOWmO+LIp36u/xk+wE9vuIL/W7xJj3txbbuq7LTLL97sLfY2gtEAJfb0EPvl3c4JikuWuNNrnUjfEzMStP4zFQ9U7hdC1btUGMnl1vufXurxmd23E6+vZbtn+2q8WmXR3ta7mjx1y6ilv6zE1cPopwO/ebSs/WXj8yHBDN9UaRTnVXvfXurT0XK8TFRemraMK+Psxq7aYAIwE4aSKdCgj9CScstqacHj32Hjqnk21o1ugL31dHWjhaX29Dwh/+h2mOdv8FdfGyUnvzlkE7vIvJ1m21Hd8ddUVapO17b7POcnr1+eEhdFWFrL2Aj75Xs1cwlJR7HPTVtqKYMPTPwE4KlOhtKmrakflhW5fFeMb7K7tdDGb26N//ZzI3wOvtFLXUcBHyxvHSfT/eHiY+J0hP/mq3EbjHNQc8wDC0vq9JJt29zuSo7TfP9VEfkL4QRwEa4MoK2NIWS5z7eoQYvr2SkJcSqsq4hIPPqTCvyzixhBKrzqLe9PwIhED1N/IE+I4CNsJMGbYlyOjRr/Lkqf/hK5Q32rl4oUEHEl90rLU3MStOm+8frr7eO1oQLUtQ12vPXWHJ8tJ69fljAWqDPmzZcid2sLcG847LA3yU4kChgBSIAO2nQkSinQwtuGKG+QWrN3p74mCifd6+0FOV0aMyg3hozqHerG/DV1J9QcvdY9ekeKzmkA0ca1KdH4O/DE+V06D+vye70EpKvusd20YyxAy352f5CGAHCHDtpYNZ9eZkacmZSp24e1xlP/OsQv4eCKKcjJJYeJ2ad2mZ99xslOtboY9GHj8Ktp0hbWKYBwlxxRU2HN8eTpENHG1VcUROkGSGUXTW0r2ZZ0Jnz9ksyQmqXRyBMzEpT2UMTlZcVvOD/m0sj4++VMAKEOe5JA2/dNW6Q4mM73yjNrFOFo5lB+3lWinI6tOBXI3TrzwYE9Od0j43Ss9cP05xJkfH3yjINEOZ2Hag3NY570qBJlNOhJ67J9mlLqjesuGV9qHjgqgvkdDj8WqNj9R14A4kwAoQxl9vQ68W7PY5jJw1ONym7r27fc7DT95Hp4pCG9k/SmUmnWrab6RdiF001Onct2aLO9tD4zaUZEXMVpC2EESCMFVfUqMrEFsxpI/vb+ksBbbsv7wK5DZm+w25T8OjXM47AYdJVQ/vK6XTozsW+7bSxy9UlwggQxszWgQzoHRfgmSBcmV1OmJydqnnThhM8fDApO03PaphmvL5FnhriXtg/Sf2S7Rf2CCNAGDNbB0K9CDpyX16mhqX3bLP1u11+Mw+0Sdl9NV8dXyGJ9KWYjhBGgDDW1Hm1vYZnDkmp1IvAhEnZaZqQlar1O79X0dcHJJ3q32GX38yDYVJ2mhY6h+uh98tbbcdPjo/WI1OybB34CCNAGKPzKvypZWdTBMbErDSNz0xVcUWN9h8+HpQOseGAMAKEKTqvAuEpVLrGhhKangFhis6rACIFYQQIU3ReBRApCCNAmGInDYBIQRgBwlTTTpr2OETnVQDhgTAChCl20gCIFIQRIAyxkwZAJCGMAGGInTQAIglhBAhD7KQBEEkII0AYYicNgEhCGAHC0MH6BnVUl8pOGgDhhHbwQJhZUVap6Yu3yMOdyNlJAyBscGUECCNNu2g6CiJOh7Tg+uGamJUWtHkBQGcQRoAwYmYXjduQesbHBGlGANB5hBEgjLCLBkAkIowAYYRdNAAiEWEECCOjMpKVlthV7ZWlsosGQDgijABhJMrp0P8aktZhASu7aACEG8IIEEZWlFXqhTUV7b7+60sz2EUDIOwQRoAwYWZb7//9vFIut6cOJAAQWggjQJgws623svY4N8cDEHYII0CYYFsvgEhFGAHCBNt6AUQqwggQJg7WN3gcw7ZeAOGIMAKEAZfb0MPLvvA47oE8tvUCCD+EESAMmClelbgnDYDwRBgBwgDFqwAiGWEECAMUrwKIZIQRIAyMykhWUlx0u69zTxoA4YwwAoSBgvIqHTra2O7rhrgnDYDwRRgBQlxTG/iOJMVFa3xmapBmBAD+RRgBQpyZnTSHjjbSBh5A2CKMACGOnTQAIh1hBAhx7KQBEOkII0CIow08gEhHGAFCGG3gAdgBYQQIYbSBB2AHhBEghFG8CsAOvA4ja9as0eTJk9W3b185HA69++67Ho/5+OOPNXz4cMXGxmrgwIF69dVXfZgqYD+7DtSbGkfxKoBw5nUYqa+v15AhQ7RgwQJT4ysqKpSXl6fLL79cJSUlmjVrlm677TZ9+OGHXk8WsBOX29Drxbs9jqN4FUC46+LtAVdeeaWuvPJK0+MXLlyojIwMPfnkk5Kkf/mXf9G6dev0l7/8RRMmTPD2xwO2UVxRo6o6zztppo3sT/EqgLAW8JqRoqIi5ebmtnpuwoQJKioqaveYhoYG1dXVtXoAdmO2DmRA77gAzwQAAivgYaSqqkopKSmtnktJSVFdXZ2OHTvW5jH5+flKTExsfqSnpwd6mkDIodkZALsIyd00c+bMUW1tbfNjz549Vk8JCDqanQGwC69rRryVmpqq6urqVs9VV1crISFB3bp1a/OY2NhYxcbGBnpqQMii2RkAOwn4lZGcnBwVFha2eq6goEA5OTmB/tFA2KLZGQA78TqMHDlyRCUlJSopKZF0autuSUmJdu8+tQVxzpw5uvHGG5vH33HHHfr66691zz336Msvv9Szzz6rv/3tb7r77rv98wmACESzMwB24nUY2bhxo4YNG6Zhw4ZJkmbPnq1hw4bpwQcflCRVVlY2BxNJysjI0LJly1RQUKAhQ4boySef1Isvvsi2XqADNDsDYCcOwzAMqyfhSV1dnRITE1VbW6uEhASrpwMElMttaMxjhR57jKQldtW6P46lZgRAyDL7/R2Su2kAO6PZGQC7IYwAIYZmZwDshjAChBianQGwG8IIEGJodgbAbggjQAih2RkAOyKMACGEZmcA7IgwAoQQmp0BsCPCCBBCKF4FYEeEESCEULwKwI4II0CIoHgVgF0RRoAQQfEqALsijAAhguJVAHZFGAFCBMWrAOyKMAKECIpXAdgVYQQIARSvArAzwggQAiheBWBnhBEgBFC8CsDOCCNACNh1oN7UOIpXAUQiwghgMZfb0OvFuz2Oo3gVQKQijAAWK66oUVWd550000b2p3gVQEQijAAWM1sHMqB3XIBnAgDWIIwAFqPZGQC7I4wAFqPZGQC7I4wAFqLZGQAQRgBL0ewMAAgjgKU+Kq8yNY5mZwAiGWEEsIjLbeidkr2mxlK8CiCSEUYAixRX1KimvtHjuF7xMRSvAohohBHAImaXXqYM7UvxKoCIRhgBLGL2fjTjM1MDPBMAsBZhBLAA96MBgB8RRgALcD8aAPgRYQSwgNktvdyPBoAdEEaAIGNLLwC0RhgBgowtvQDQGmEECLIX1+40NY4tvQDsgjACBNHy0n0q/PI7U2PZ0gvALggjQJC43Ib+8FapqbEs0QCwE8IIECTzV25XfYPL1FiWaADYCWEECAKX29Arn+wyPZ4lGgB2QhgBgqC4okaHjnneQSOxRAPAfggjQBCYvSmeJD08JYslGgC2QhgBgsDsTfGuyk7TpOy0AM8GAEILYQQIMJfb0MufVHgcl9iti56aNiwIMwKA0EIYAQLsmcLtqj120uO4W8aczfIMAFsijAABtLx0n54q3G5qLDfFA2BXXayeABCpVpRV6s7FW0yP56Z4AOyKKyNAALjchh56v9z0+KS4aLbzArAtwggQAMUVNaqsNb+d9+aLM6gXAWBbhBEgALzpK5IUF60ZYwcGcDYAENoII0AAmO0rIkmPXT2YqyIAbI0wAviZy23o9eLdHsc5HdKz1w/XxCyanAGwN8II4GfFFTWqqmvwOO6usYPotgoAIowAfvdReZWpcWefER/gmQBAeCCMAH7kcht6p2SvqbH0FQGAUwgjgB8VV9Sopr7R47he8TH0FQGAHxBGAD8yu6V3ytC+7KABgB8QRgA/Mruld3xmaoBnAgDhgzAC+InZLb1piV1ZogGAFggjgJ+Y3dI7bWR/lmgAoAXCCOAnZutFBvSOC/BMACC8EEYAPzG7VZctvQDQGmEE8JOD9Z6XaKgXAYCfIowAfuByG3p42Rcexz2Ql0m9CACchjAC+EFxRY0qaz3XjPSMjwnCbAAgvBBGAD8wW7xqdhwA2IlPYWTBggUaMGCAunbtqtGjR6u4uLjdsa+++qocDkerR9euFPAhslC8CgC+8zqMvPHGG5o9e7bmzp2rzZs3a8iQIZowYYL279/f7jEJCQmqrKxsfnzzzTedmjQQakZlJCspLrrd1x2ieBUA2uN1GPnzn/+s22+/XTfffLMyMzO1cOFCxcXF6eWXX273GIfDodTU1OZHSkpKpyYNhJqC8iodOtr+DfIMSXMnU7wKAG3xKoycOHFCmzZtUm5u7o9v4HQqNzdXRUVF7R535MgRnXXWWUpPT9eUKVO0bdu2Dn9OQ0OD6urqWj2AUOVyG3ro/fIOxyTFRXM/GgBoh1dh5MCBA3K5XD+5spGSkqKqqqo2jznvvPP08ssv67333tNrr70mt9utiy++WN9++227Pyc/P1+JiYnNj/T0dG+mCQSVmZ00h442qriiJkgzAoDwEvDdNDk5Obrxxhs1dOhQXXbZZXr77bd1xhln6Pnnn2/3mDlz5qi2trb5sWfPnkBPE/AZO2kAoHO6eDO4d+/eioqKUnV1davnq6urlZpq7hJ0dHS0hg0bph07drQ7JjY2VrGxsd5MDbAMO2kAoHO8ujISExOjESNGqLCwsPk5t9utwsJC5eTkmHoPl8ulrVu3Ki0tzbuZAiGKnTQA0DleXRmRpNmzZ+umm27ShRdeqFGjRmnevHmqr6/XzTffLEm68cYbdeaZZyo/P1+S9B//8R+66KKLNHDgQB06dEhPPPGEvvnmG912223+/SSARdhJAwCd43UYufbaa/Xdd9/pwQcfVFVVlYYOHaoVK1Y0F7Xu3r1bTuePF1wOHjyo22+/XVVVVerZs6dGjBihTz/9VJmZmf77FIBF2EkDAJ3nMAzDsHoSntTV1SkxMVG1tbVKSEiwejpAs6Kd3+u6Res9jnv99ouUc06vIMwIAEKH2e9v7k0DdAI7aQCg8wgjQCfsOlBvahw7aQCgfYQRwEcut6HXi3d7HMdOGgDoGGEE8FFxRY2q6ho8jps2sj87aQCgA4QRwEdm60AG9I4L8EwAILwRRgAfUS8CAP5BGAF8QL0IAPgPYQTwAfUiAOA/hBHAB9SLAID/EEYAH1AvAgD+QxgBvES9CAD4F2EE8BL1IgDgX4QRwEvUiwCAfxFGAC+ZrQOhXgQAzCGMAF46WO95iYZ6EQAwjzACeMHlNvTwsi88jnsgL5N6EQAwiTACeKG4okaVtZ5rRnrGxwRhNgAQGQgjgBfMFq+aHQcAIIwAXqHZGQD4H2EEMIlmZwAQGIQRwCSanQFAYBBGAJNodgYAgUEYAUyiXgQAAoMwAphAvQgABA5hBDCBehEACBzCCGAC9SIAEDiEEcAEbo4HAIFDGAFMGJWRrKS46HZfd4h6EQDwFWEEMKGgvEqHjja2+7ohae5kbo4HAL4gjAAeuNyGHnq/vMMxSXHRGp+ZGqQZAUBkIYwAHpi5U++ho40qrqgJ0owAILIQRgAPuFMvAAQWYQTwgM6rABBYhBGgA3ReBYDAI4wAHaDzKgAEHmEE6ACdVwEg8AgjQAfovAoAgUcYATpwsN7zEg31IgDQOYQRoB0ut6GHl33hcdwDeXReBYDOIIwA7TDT7EySesbHBGE2ABC5CCNAO2h2BgDBQRgB2kHxKgAEB2EEaAfFqwAQHIQRoA0UrwJA8BBGgDZQvAoAwUMYAdpA8SoABA9hBGgDxasAEDyEEaANFK8CQPAQRoDTULwKAMFFGAFOQ/EqAAQXYQQ4zUflVabGUbwKAP5BGAFacLkNvVOy19RYilcBwD8II0ALxRU1qqlv9DiuV3wMxasA4CeEEaAFs0s0U4b2pXgVAPyEMAL8wJslmvGZqQGeDQDYB2EE+AFLNABgDcII8AOWaADAGoQRQNKKskq99MkuU2NZogEA/yKMwPZcbkMPvV9uaiwt4AHA/wgjsD2zHVclae5kWsADgL8RRmB7Zjup3jJmgCZmpQV4NgBgP4QR2N6uA/WmxlErAgCBQRiBrbnchl4v3u1xHLUiABA4hBHYWnFFjarqGjyOmzayP7UiABAghBHYmtneIgN6xwV4JgBgX4QR2BZ36AWA0OBTGFmwYIEGDBigrl27avTo0SouLu5w/Jtvvqnzzz9fXbt21eDBg7V8+XKfJgv40/qvv6f9OwCEAK/DyBtvvKHZs2dr7ty52rx5s4YMGaIJEyZo//79bY7/9NNPdd111+nWW2/Vli1bNHXqVE2dOlVlZWWdnjzgq+Wllbrl1c9MjaX9OwAElsMwDMObA0aPHq2RI0dq/vz5kiS326309HTddddduvfee38y/tprr1V9fb2WLl3a/NxFF12koUOHauHChaZ+Zl1dnRITE1VbW6uEhARvpgv8xKPLyrVobYXp8a/ffpFyzukVwBkBQGQy+/3t1ZWREydOaNOmTcrNzf3xDZxO5ebmqqioqM1jioqKWo2XpAkTJrQ7XpIaGhpUV1fX6gF0lsttaPprm7wKIizRAEDgeRVGDhw4IJfLpZSUlFbPp6SkqKqq7V0JVVVVXo2XpPz8fCUmJjY/0tPTvZkm8BMryiqVNXeFlpWZ2z3ThCUaAAi8kNxNM2fOHNXW1jY/9uzZY/WUEMZWlFXqjtc261ij2+tj6boKAIHXxZvBvXv3VlRUlKqrq1s9X11drdTUtv+nnZqa6tV4SYqNjVVsbKw3U0MIcrkNrd/5vYq+PiDJoZxzeumis3sF9UqDy23oj2+V+nRsUlw0SzQAEARehZGYmBiNGDFChYWFmjp1qqRTBayFhYWaMWNGm8fk5OSosLBQs2bNan6uoKBAOTk5Pk8a1jo9ZIzOSJbT6dCBIw3qHR8rOU41E3v9sz063uJqxPxVOxQfG6UnrsnWpOy+QZnr/JXbVXvspE/H3nxxBks0ABAEXoURSZo9e7ZuuukmXXjhhRo1apTmzZun+vp63XzzzZKkG2+8UWeeeaby8/MlSTNnztRll12mJ598Unl5eVqyZIk2btyoF154wb+fJICavnw/2fmd9h48JofDoTN7dtPF5/QO+m/6vmoZINyGlNgtWnXHG+Xw8orF8tJK3fNWqY40/PgFP3+V+XnUN7h05+ItGr72a909/jxJ0oaK7xWIKycut6GXP9nl07FJcdGaMXagX+YBAOiY12Hk2muv1XfffacHH3xQVVVVGjp0qFasWNFcpLp79245nT+Wolx88cVavHix7r//fv3pT3/SoEGD9O677yorK8t/nyKA2vrybbJg1c6g/6bvi44+g3TqikVsF4cuP6+PBvbp0W4o8HZLbEc276nVv73culmev6+cPFO4XbXHPDc1a8tjVw8Oi5AJAJHA6z4jVghmn5GWV0E+3Falnd8dNXXcby7N0JxJmQGdW1s81WXkLy/X82u8DxBJcdF67OrBmpiVJpfb0O8Wb/Z6J0pn3H7JAN2Xd4FXx/h67lpKS+yquZMzNTErzetjAQCtmf3+Joz8wOU2NH/lDj2/ZqeOnnD59B7PXj9ck7KD9yXW3hWPpiDhdhu6c/GWTv2Mc86IU8WBo3Jb8G/JrT8boAeuMhdIVpRV6t63t+rQUd+uhFx+Xm/9+tKBGpWRzBURAPATwogXPC1jmJUcF63P7h8f8C8zl9vQzCVbtLS0ssNxXbs4dfyk99tZQ8ntl2TovryOrzg1bd311fxpw3TV0NBdZgOAcGX2+9vrmpFI4XIbKq6o0QtrdmrVV9/55T1rjjbq6cLtunv8uX55v7asKKvUH98qNbVDJNyDiCQtWluh+JguyjgjXn16dP3JlYvObN2VpH+/+CyCCABYzJZhZEVZpR56v1yVtcf9/t5PFW7X0RONXtc7mNHZKwDhal7h9uZ/7hrt1M/PPUP/ljNAF53dS797fbPPW3clacIF1IYAgNVsF0ZWlFXqt69tViDXphat3aXPKmr0h4n/4retqidOuvX/ven7FYBIcbzRrRXbqrViW7WcDnWqliUtsStNzQAgBNgqjLjchh56vzygQaRJybd1uuHFDZ3equpyG3qmcLueWbldrpCv7gmuzhbVzp2cSbEqAISAkLw3TaAUV9QEZGmmI01Nvh5dts2r41xuQ/MK/qnz7/9A8wrDO4j0jOuiqwanKqZLaPzrFh8bpYW/Gs72XQAIEba6MrL/cHCDSEuL1u6S21DzVtWO+oMsL63U7L+VhEUB6qSsPjr7jB6turoahtQzLka9e8QqNeHHotOmqzwLV++07LPlDU7R09eN4IoIAIQQW4WRPj26dur4+BinukQ5fS6YfGndLknSiP7JbbRUP9UF9YK0BG3eU9upeZpxZVaqzkvpof9e/41q6k94fXzXaKf+/MshXi0/RTkdmjX+XN01blCrIPZZxffasOug13PwRvfYKD0e4p1yAcCubNVnxOU2NPLRAtXUm2uMdXbvOGX3S2p1H5qC8qqw39HSPbaLPp97RfPViuKKGhWUV+ndkn0eg0lsF4d+e9k5umvcuX69h0z2Qx+qvsG3ZnMdmZCZohsvHhA29xACgEhC07N2LC/dZ6oraUfdP1eUVeruN0p0rDH0l1Ha0l6n2KZgUlV7TAeONKjm6AntC9KNAc2eF2+YaZgGAAgcwkgHPN2vxcx9Uay4X0tHbhuTodc2fOOxFsOqe+iY4et9dNriy71tAAD+RRjxYHlppe5/r6zVskRyfLQemZLlVV2BP+9k66umgNFRgWi41Ex0tjV/uHxOALADwogJTcsS+w8fb7PVuFn+ureNt9orIvV0J99Q1/Luu/sOHVdaUlcldYvRB2WVKumguHdydqrmTRseNp8TACIdYSTImr5AH//wC33+bV1Af1Z0lEPTf+7fItJw4a8rWgCAwCOMWOjhpduat/H6k51DSEv+uqIFAAgs7tproQeuukBOh8OvtSSTslL0zPU065JO9SvJOaeX1dMAAPhJaPTnjkD35WXq2euHq3ts5/Pe7ZcM0LO/upAgAgCISISRAJqUnabP516hu3PPVVxMVJtj4mOcmpSVoq5t3Lele2yUnr1+GFtUAQARjZqRIDl9h8jpTcTCfQcMAACno4AVAABYyuz3N8s0AADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBSYXHX3qYmsXV1dRbPBAAAmNX0ve2p2XtYhJHDhw9LktLT0y2eCQAA8Nbhw4eVmJjY7uthcW8at9utffv2qUePHnI4/HfjuLq6OqWnp2vPnj3c8yZEcY5CH+co9HGOQl+kniPDMHT48GH17dtXTmf7lSFhcWXE6XSqX79+AXv/hISEiDr5kYhzFPo4R6GPcxT6IvEcdXRFpAkFrAAAwFKEEQAAYClbh5HY2FjNnTtXsbGxVk8F7eAchT7OUejjHIU+u5+jsChgBQAAkcvWV0YAAID1CCMAAMBShBEAAGApwggAALCUrcPIggULNGDAAHXt2lWjR49WcXGx1VOyjTVr1mjy5Mnq27evHA6H3n333VavG4ahBx98UGlpaerWrZtyc3O1ffv2VmNqamp0ww03KCEhQUlJSbr11lt15MiRIH6KyJWfn6+RI0eqR48e6tOnj6ZOnaqvvvqq1Zjjx49r+vTp6tWrl7p3765rrrlG1dXVrcbs3r1beXl5iouLU58+ffSHP/xBJ0+eDOZHiVjPPfecsrOzm5tk5eTk6IMPPmh+nfMTWh577DE5HA7NmjWr+TnO0Y9sG0beeOMNzZ49W3PnztXmzZs1ZMgQTZgwQfv377d6arZQX1+vIUOGaMGCBW2+/vjjj+vpp5/WwoULtWHDBsXHx2vChAk6fvx485gbbrhB27ZtU0FBgZYuXao1a9bo17/+dbA+QkRbvXq1pk+frvXr16ugoECNjY264oorVF9f3zzm7rvv1vvvv68333xTq1ev1r59+3T11Vc3v+5yuZSXl6cTJ07o008/1X/913/p1Vdf1YMPPmjFR4o4/fr102OPPaZNmzZp48aNGjt2rKZMmaJt27ZJ4vyEks8++0zPP/+8srOzWz3POWrBsKlRo0YZ06dPb/6zy+Uy+vbta+Tn51s4K3uSZLzzzjvNf3a73UZqaqrxxBNPND936NAhIzY21nj99dcNwzCM8vJyQ5Lx2WefNY/54IMPDIfDYezduzdoc7eL/fv3G5KM1atXG4Zx6nxER0cbb775ZvOYL774wpBkFBUVGYZhGMuXLzecTqdRVVXVPOa5554zEhISjIaGhuB+AJvo2bOn8eKLL3J+Qsjhw4eNQYMGGQUFBcZll11mzJw50zAM/hs6nS2vjJw4cUKbNm1Sbm5u83NOp1O5ubkqKiqycGaQpIqKClVVVbU6P4mJiRo9enTz+SkqKlJSUpIuvPDC5jG5ublyOp3asGFD0Occ6WprayVJycnJkqRNmzapsbGx1Tk6//zz1b9//1bnaPDgwUpJSWkeM2HCBNXV1TX/9g7/cLlcWrJkierr65WTk8P5CSHTp09XXl5eq3Mh8d/Q6cLiRnn+duDAAblcrlYnWJJSUlL05ZdfWjQrNKmqqpKkNs9P02tVVVXq06dPq9e7dOmi5OTk5jHwD7fbrVmzZmnMmDHKysqSdOrvPyYmRklJSa3Gnn6O2jqHTa+h87Zu3aqcnBwdP35c3bt31zvvvKPMzEyVlJRwfkLAkiVLtHnzZn322Wc/eY3/hlqzZRgBYN706dNVVlamdevWWT0VnOa8885TSUmJamtr9fe//1033XSTVq9ebfW0IGnPnj2aOXOmCgoK1LVrV6unE/JsuUzTu3dvRUVF/aRqubq6WqmpqRbNCk2azkFH5yc1NfUnxcYnT55UTU0N59CPZsyYoaVLl2rVqlXq169f8/Opqak6ceKEDh061Gr86eeorXPY9Bo6LyYmRgMHDtSIESOUn5+vIUOG6KmnnuL8hIBNmzZp//79Gj58uLp06aIuXbpo9erVevrpp9WlSxelpKRwjlqwZRiJiYnRiBEjVFhY2Pyc2+1WYWGhcnJyLJwZJCkjI0Opqamtzk9dXZ02bNjQfH5ycnJ06NAhbdq0qXnMypUr5Xa7NXr06KDPOdIYhqEZM2bonXfe0cqVK5WRkdHq9REjRig6OrrVOfrqq6+0e/fuVudo69atrUJjQUGBEhISlJmZGZwPYjNut1sNDQ2cnxAwbtw4bd26VSUlJc2PCy+8UDfccEPzP3OOWrC6gtYqS5YsMWJjY41XX33VKC8vN379618bSUlJraqWETiHDx82tmzZYmzZssWQZPz5z382tmzZYnzzzTeGYRjGY489ZiQlJRnvvfeeUVpaakyZMsXIyMgwjh071vweEydONIYNG2Zs2LDBWLdunTFo0CDjuuuus+ojRZTf/va3RmJiovHxxx8blZWVzY+jR482j7njjjuM/v37GytXrjQ2btxo5OTkGDk5Oc2vnzx50sjKyjKuuOIKo6SkxFixYoVxxhlnGHPmzLHiI0Wce++911i9erVRUVFhlJaWGvfee6/hcDiMf/zjH4ZhcH5CUcvdNIbBOWrJtmHEMAzjmWeeMfr372/ExMQYo0aNMtavX2/1lGxj1apVhqSfPG666SbDME5t733ggQeMlJQUIzY21hg3bpzx1VdftXqP77//3rjuuuuM7t27GwkJCcbNN99sHD582IJPE3naOjeSjFdeeaV5zLFjx4w777zT6NmzpxEXF2f84he/MCorK1u9z65du4wrr7zS6Natm9G7d2/j97//vdHY2BjkTxOZbrnlFuOss84yYmJijDPOOMMYN25ccxAxDM5PKDo9jHCOfuQwDMOw5poMAACATWtGAABA6CCMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABLEUYAAIClCCMAAMBS/w/3Qk0My4/H0wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict_regional_temperatures(global_test_series, slopes, intercepts, same_shape = False):\n",
    "    \"\"\"\n",
    "    Uses stored regression parameters to predict regional temps.\n",
    "    Returns:\n",
    "        predictions: array of shape (len(global_test_series), n_regions)\n",
    "    \"\"\"\n",
    "    original_shape = np.asarray(global_test_series).shape\n",
    "    global_test_series = np.asarray(global_test_series).reshape(-1, 1)\n",
    "    predictions = global_test_series * slopes + intercepts\n",
    "\n",
    "    if same_shape:\n",
    "        # Collapse regional dimension (e.g., mean or first region)\n",
    "        # Here: return full region predictions but reshaped like input\n",
    "        # so we return an array where the last dimension = n_regions.\n",
    "        return predictions.reshape(*original_shape, -1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def fit_regional_regressions(global_series, regional_series):\n",
    "    \"\"\"\n",
    "    Fits one linear regression per region: region = a + b * global\n",
    "    Returns:\n",
    "        slopes:   (46,) array of regression coefficients\n",
    "        intercepts: (46,) array of intercepts\n",
    "    \"\"\"\n",
    "    global_series = np.asarray(global_series).reshape(-1, 1)\n",
    "    regional_series = np.asarray(regional_series)\n",
    "    \n",
    "    n_regions = regional_series.shape[1]\n",
    "    slopes = np.zeros(n_regions)\n",
    "    intercepts = np.zeros(n_regions)\n",
    "\n",
    "    for i in range(n_regions):\n",
    "        model = LinearRegression()\n",
    "        model.fit(global_series, regional_series[:, i])\n",
    "        slopes[i] = model.coef_[0]\n",
    "        intercepts[i] = model.intercept_\n",
    "\n",
    "    return slopes, intercepts\n",
    "\n",
    "#print(gmt_train)\n",
    "#print(gmt_train.shape)\n",
    "#print(train_data_regional_temperature)\n",
    "#print(train_data_regional_temperature.shape)\n",
    "#slopes, intercepts = fit_regional_regressions(gmt_train, train_data_regional_temperature.transpose(1,0))\n",
    "pattern_scaling_prediction = predict_regional_temperatures(gmt_autoregressive_test, slope, intercept).T\n",
    "print(pattern_scaling_prediction.shape)\n",
    "\n",
    "plt.scatter(np.arange(len(pattern_scaling_prediction[14,:])) , pattern_scaling_prediction[14,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 30, 34400)\n",
      "(58, 34400)\n",
      "(59, 30, 400)\n",
      "(58, 400)\n",
      "(58, 30, 34400)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data_shuffeled[0].transpose(2,0,1)#\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = train_data_shuffeled[1].transpose(1,0)\n",
    "\n",
    "X_test = test_data_input.transpose(2,0,1)#\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "Y_test = test_data_output.transpose(1,0)\n",
    "\n",
    "print(train_data_shuffeled[0].shape)\n",
    "\n",
    "print(train_data_shuffeled[1].shape)\n",
    "\n",
    "print(test_data_input.shape)\n",
    "\n",
    "print(test_data_output.shape)\n",
    "\n",
    "ps_preds = predict_regional_temperatures(train_data_shuffeled[0][0,:,:], slopes, intercepts, same_shape = True).transpose(2,0,1)\n",
    "print(ps_preds.shape)\n",
    "\n",
    "residuals = np.zeros(train_data_shuffeled[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "downscaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
